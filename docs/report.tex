%	DOCUMENT TYPE
\documentclass[notitlepage, 11pt, nofootinbib]{revtex4-1}

%	PACKAGES
\usepackage[utf8]{inputenc}
\usepackage{amssymb,amsmath,amsfonts,mathtools,dsfont,bm,enumitem}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage[pdftex]{graphicx}
\usepackage[pdftex,plainpages=false,colorlinks=true,linkcolor=Red, citecolor=blue, urlcolor=blue]{hyperref}
\usepackage[most]{tcolorbox}
\usepackage{empheq}
\usepackage{listings}
\lstset{language=python}

%	FONT & CUSTOM SYMBOLS
%\usepackage{esint}
%\usepackage[T1]{fontenc}
%\usepackage{microtype}
% \usepackage[fullfamily,lf,minionint,openg,loosequotes]{MinionPro}
% \usepackage{libertine}
% \usepackage{txfonts}

%%%% vector (bold)
% \renewcommand{\vec}[1]{\bm{\mathrm{#1}}}
\renewcommand{\vec}[1]{\bm{#1}}

%%%% tensor
% \usepackage{stackengine}\stackMath
% \def\lvec{{\rotatebox{180}{$\mkern+0mu\mathchar"017E$}}}
% \def\tensign{\smash{\stackon[-1.99pt]{\mkern-4mu\mathchar"017E}{\rotatebox{180}{$\mkern+0mu\mathchar"017E$}}}}
% \def\tensor#1{\def\useanchorwidth{T}\stackon[-4.3pt]{#1}{\,\tensign}}


%	DOCUMENT
\begin{document}

\title{\bf Temperature-agnostic analytic continuation of conductivity\\and demonstration with neural networks}
\author{S. \surname{Verret}}
\affiliation{Institute for Learning Algorithms (MILA), Montr\'eal, Canada}
\affiliation{Institut Quantique (IQ), Sherbrooke, Canada}
\date{\today}
% \keywords{}

\begin{abstract}

\end{abstract}

\maketitle
\vspace{-.5cm}
\tableofcontents



\section{Introduction}


\section{Temperature-agnostic analytic continuation}

\subsection{Analytic continuation of conductivity}

The starting point of this paper is the following relation (motivated in appendix) between the Matsubara response function $\Pi(i\omega_n)$ and the optical conductivity $\sigma(\omega)$,
\begin{align}
\Pi(i\omega_n) 
=
\int_{-\infty}^{\infty} \frac{d\omega}{\pi} \frac{\omega^2}{\omega^2+\omega_n^2}
\text{Re}\{ \sigma(\omega) \}.
\label{Pi_continuation}
\end{align}
Here, $\omega$ is a frequency or energy (we set $\hbar=1$), $\omega_n=2\pi k_BTn$ are Matsubara frequencies, $T$ is the temperature, and $k_B$ is the Boltzmann constant.
Four our purpose, the term ``analytic continuation'' refers to the problem of inverting equation~\eqref{Pi_continuation}. That is, recover a numerical estimate of $\text{Re}\{ \sigma(\omega) \}$ given a numerical estimate for $\Pi(i\omega_n)$.

Two special cases of the above equation will be of particular importance. First, the value $\Pi(i\omega_n=0)$ which corresponds to the normalization of $\text{Re}\{ \sigma(\omega) \}$,
\begin{align}
\Pi(0) 
=
\int_{-\infty}^{\infty} \frac{d\omega}{\pi}
\text{Re}\{ \sigma(\omega) \}.
\label{Pi_normalization}
\end{align}
Second, values of $\Pi(i\omega_n)$ at high $\omega_n$ and values of $\text{Re}\{ \sigma(\omega) \}$ at high $\omega$ (the tails) are related by
\begin{align}
\lim_{\omega_n\rightarrow\infty}
\omega_n^2
\Pi(i\omega_n)
=
\int_{-\infty}^{\infty} \frac{d\omega}{\pi}\omega^2 
\text{Re}\{ \sigma(\omega) \}.
\label{Pi_moment}
\end{align}

\subsection{Dimensionless formulation}
\label{sec_dimensionless}
We can obtain a dimensionless version of~\eqref{Pi_continuation} by dividing it by~\eqref{Pi_normalization}. To clarify the dimensional analysis,we substitute $\omega_n = 2\pi k_BTn$, and temporarily restore $\hbar$,
\begin{align}
\frac{\Pi(i\omega_n)}{\Pi(0)}
&=
\int_{-\infty}^{\infty} d\omega \frac{\omega^2}{
\omega^2+
\big(\frac{2\pi k_B}{\hbar}T\big)
^2n^2
}\left(
\frac{\text{Re}\{ \sigma(\omega) \}}
{\int_{-\infty}^{\infty} d\omega \text{Re}\{ \sigma(\omega) \}}
\right).
\label{dimensionless}
\end{align}
We will denote the left-hand side as a vector $\vec \Pi$, \begin{align}
\Pi_n = \frac{\Pi(i\omega_n)}{\Pi(0)} 
% = \frac{\Pi(i2\pi k_B T n)}{\Pi(0)},
\end{align}
and the term in parenthesis as a density function $p(\omega)$,
\begin{align}
p(\omega) =
\frac{\text{Re}\{ \sigma(\omega) \}}
{\int_{-\infty}^{\infty} d\omega \text{Re}\{ \sigma(\omega) \}}.
\label{eq_def_density}
\end{align}
These are natural dimensionless input $\vec\Pi$ and output $p(\omega)$ for the analytic continuation problem. We then rewrite \eqref{dimensionless} with a change of variable $\omega\rightarrow s\omega$
\begin{align}
\Pi_n
&=
\int_{-\infty}^{\infty} d\omega \frac{\omega^2}{\omega^2 + \left(\frac{2\pi k_B}{\hbar}T\right)^2n^2}
p(\omega),
\label{dimensionless_with_p1}
\\
&=
\int_{-\infty}^{\infty} d\omega \frac{\omega^2}{\omega^2 + \left(\frac{2\pi k_B}{\hbar}\frac{T}{s}\right)^2n^2}
sp(s\omega),
\label{dimensionless_with_p2}
\end{align}
which lets us isolate an alternative integrand argument $sp(s\omega)$, where $s$ fits the definition of a \emph{scale parameter}.
If we consider that $s$ carries the units of frequency, $\omega$ becomes a dimensionles continuous variable (analogous to the integer $n$ in matsubara frequencies) and the term $\frac{2\pi k_B}{\hbar}\frac{T}{s}$ then corresponds to a dimensionless temperature.

\subsection{Temperature-agnostic formulation}
\label{section_temperature_agnostic}
Equations~\eqref{dimensionless_with_p1} and~\eqref{dimensionless_with_p2} reveal a degenerancy between analytic continuation problems at different temperatures.
That degenerancy can be expressed explicitly by writing integral~\eqref{dimensionless_with_p1} as a functional function $\Pi_n = \Pi_n[p(\omega)](T)$, such that lines~\eqref{dimensionless_with_p1} and~\eqref{dimensionless_with_p2} become 
\begin{align}
\Pi_n[p(\omega)](T) = \Pi_n[sp(s\omega)](T/s).
\label{degenerancy}
\end{align}
This equality tells us that the same response function components $\Pi_n$ can be found at different temperatures $T$ and $T/s$ for respective distributions $p(\omega)$ and $sp(s\omega)$. 
This means that if neither $T$ nor $s$ is specified, the problem input $\vec \Pi$ has a continuum of compatible solutions $sp(s\omega)$, where~$s$ can take any value, and thus, the analytic continuation task is ambiguous.
For a well defined problem, either $T$ or $s$ must be fixed.
On the flipside, it also means that once the solution $p(\omega)$ is known at fixed or $T$ or $s$, then one can obtain the solution at any other $T$ or $s$ by a simple rescaling of the form $sp(s\omega)$.
We call such strategy \emph{temperature-agnostic analytic continuation}.

\subsubsection{Fixing $T$}
The first way to obtain temperature-agnostic analytic continuation is to work from a single temperature $T_\text{ref}$. First, one obtains the solution at temperature $T_\text{ref}$. Then, with~\eqref{degenerancy} rewritten as
\begin{align}
    \Pi_n[p(\omega)](T_\text{{ref}}) = \Pi_n[\tfrac{T_{\text{ref}}}{T} p(\tfrac{T_{\text{ref}}}{T}\omega)](T),
    \label{eq_fix_T}
\end{align}
we see that the solution $p(\omega)$ obtained at $T_\text{ref}$ can be rescale to obtain the solution $\tfrac{T_{\text{ref}}}{T} p(\tfrac{T_{\text{ref}}}{T}\omega)$, valid at any other $T$. It is the easiest way to get temperature-agnostic analytic continuation.

\subsubsection{Fixing $s$}
\label{sec_usage}
The second way to obtain temperature-agnostic analytic continuation is to work with fixed~$s$. As we will argue in the rest of this paper, this fixed-$s$ approach provides several desirable advantages over the simpler fixed-$T$ approach, especially when training neural networks. Fixing $s$ requires to modify the goal of analytic continuation. Instead of searching for the particular $sp(s\omega)$ associated at temperature $T/s$, we shall ignore temperature, and choose to always search for $s_\text{ref}p(s_\text{ref}\omega)$, for some predetermined $s_\text{ref}$. Without loss of generality, we can set $s_\text{ref}=1$ and thus analytic continuation becomes:
\begin{align}
    \Pi_n[sp(s\omega)](T/s) &\longrightarrow p(\omega) &\text{for any }s.
    \label{eq_agnostic_ac}
\end{align}
We will denote the target $p(\omega)$ as the \emph{reference} density from now on.

We shall now show that one can always recover the missing $s$, and thus the desired $sp(s\omega)$ from the information contained in $\vec\Pi$ and $T$ alone.
To show this, let us suppose that the fixed-$s$ solver we just described already exist, namely a function $\hat{p}(\omega, \vec \Pi)$ (it can, but doesn't need to be, a neural network), which takes $\vec \Pi$ as an input and yields the reference $\hat{p}(\omega, \vec \Pi) \approx p(\omega)$ as an output. To perform analytic continuation, the user starts from a response function $\Pi(i\omega_n)$ and prepares the according dimensionless input vector $\vec \Pi$, with
\begin{align}
\Pi_n = \frac{\Pi(i\omega_n)}{\Pi(0)}.
\end{align}
Using the latter as an input for the solver, the user obtains the reference $\hat{p}(\omega, \vec \Pi)\approx p(\omega)$. However, the user desires not the reference distribution $p(\omega)$, but one of the many possible $sp(s\omega)$, with the particular $s$ that comes for their temperature.
We can isolate that $s$ with a change of variable $s\omega\rightarrow \omega$ in~\eqref{Pi_moment},
\begin{align}
\lim_{\omega_n\rightarrow\infty}\omega_n^2
\frac{\Pi(i\omega_n)}{\Pi(0)}
&=
\int_{-\infty}^{\infty}d\omega \omega^2 sp(s\omega)
\\
&=
\frac{1}{s^2}
\int\limits_{-\infty}^{\infty}d\omega \omega^2 p(\omega).
\end{align}
leading to 
\begin{align}
s
&=
\sqrt{
\frac{1}{\mu_2}
\lim\limits_{\omega_n\rightarrow\infty}\omega_n^2
\dfrac{\Pi(i\omega_n)}{\Pi(0)}
},
&\text{with }
\mu_2
=
\int d\omega \omega^2 p(\omega).
\label{user_s}
\end{align}
where $\mu_2$ is the second central moment of the output density.\footnote{The solver can be designed such that $\mu_2$ is constant, as done for neural networks in section~\ref{sec_data_rescaling}}
The desired conductivity can now be recovered by putting the $s$ found above in the output of the solver $sp(s\omega) \approx s\hat{p}(s\omega, \vec \Pi)$ and combining it with~\eqref{Pi_normalization} and~\eqref{eq_def_density},
\begin{align}
\text{Re}\{ \sigma(\omega) \} = \pi\Pi(0) 
s \hat{p}(s\omega, \vec \Pi).
\end{align}
Since \eqref{user_s}~depends on the user's temperature through $\omega_n = 2\pi n k_B T$, analytic continuation is then possible for any temperature, all we need is the fixed-$s$ solver $\hat{p}(\omega, \vec \Pi)\approx p(\omega)$ we supposed existed. We will now explain how neural networks can realize such fixed-$s$ solvers.


\section{Demonstration with neural networks}


\subsection{Discrete formulation}
\label{section_discrete}

Analytic continuation with neural networks is typically done with vector-to-vector neural networks~\cite{Fournier2020,Xie2019,Yoon2018,Kades2019}. The output of the neural network is then a vector $\hat{\vec p} = p_{\vec \theta}(\vec \Pi)$ which targets the desired function $p(\omega)$ on a grid of real frequencies,
\begin{align}
\omega_m &=
\Delta\omega m.
&\text{with }m\in\{-M,...1,0,1,...,M\}.
\label{eq_discrete_grid}
\end{align}
the spacing between frequencies $\Delta \omega$ can also be expressed as $\Delta\omega =\omega_{\text{max}}/M$ with $\omega_{\text{max}}$ marking the end of the sampling. Discretizing~\eqref{dimensionless_with_p1} on this grid leads to
\footnote{One must be careful and remember that discretizing integrals leads to an approximation. 
As such, we recommend to avoid sums like~\eqref{discretized_continuation} and perform more accurate integration of $p(\omega)$ whenever possible.
In fact, seeing the analytic continuation kernel as a matrix and inverting that matrix is not the same as solving the continuous inversion problem. This can be understood by imagining the limiting case in which one uses only a handful of points to approximate the integral. Such a coarse discretization would contain additional difficulty not found in the continuous problem. Equating the matrix inversion and the integral inversion can therefore be misleading.
}
\begin{align}
[\vec \Pi]_n
&\approx
\sum_{m}
\Delta\omega \frac{\omega_m^2}{\omega_m^2+\omega_n^2} p(\omega_m)
\label{discretized_continuation_1}
\\
&\approx
\sum_{m}
\frac{m^2}{m^2+(\frac{2\pi k_B}{\hbar}\frac{T}{\Delta\omega}n)^2} \Delta\omega p(\Delta\omega m)
\\
&\approx
\sum_{m}
\frac{m^2}{m^2+(\frac{2\pi k_B}{\hbar}\frac{T}{\Delta\omega}n)^2} 
[\vec p]_m
\label{discretized_continuation}
\end{align}
where we define the target vector $\vec p$ as
    \footnote{Note that this definition ensures normalization of $\vec p$, as seen from the discretized integral,
    \begin{align}
    \int d\omega p(\omega) = 1 \approx \sum_m \Delta\omega p(\Delta\omega m).
    \end{align}
    This is also true for any rescaled $p_0(\omega)= sp(s\omega)$, as we can see from
    \begin{align}
        [\vec p_0]_m = \Delta\omega p_0(\Delta\omega m) = s\Delta\omega p(s\Delta\omega m).
    \end{align}
    Both $\vec p$ and $\vec p_0$ sum to one, because both $p(s)$ and $sp(s\omega)$ integrate to one. The difference between the two is that the frequencies at which $p(\omega)$ is evaluated are different; respectively $\Delta\omega m$ and $s\Delta\omega m$.}
\begin{align}
    [\vec p]_m = \Delta\omega p(\Delta\omega m).
\end{align}
Training the neural network in this framework amounts to minimize the average distance between~$\hat{\vec p}$ and~$\vec p$ over a training dataset.
% Section~\ref{sec_training} provides an example of how this can be done.

\subsection{Generting random densities}
\label{sec_distribution}
The training dataset consists of a large number of pairs of inputs $\vec \Pi$ and output targets $p(\omega)$. To create each ($\vec \Pi$, $p(\omega)$) pair, we start by generating arbitrary densities $p_0(\omega)$. 
Since $p_0(\omega)$ is generated randomly it is scaled by some random unknown~$s$ compared to its reference density~$p(\omega)$;
\begin{align}
    p_0(\omega)=sp(s\omega). 
\end{align}
This section briefly explains how to generate random densities $p_0(\omega)$. Section~\ref{sec_Pi} explains how to prepare $\vec \Pi$ and how to remove the inherent scale to prepare the reference $p(\omega)$ from those arbitrary density. Note that, to keep the discussion general, the densities $p_0(\omega)$ and $p(\omega)$ prepared are always functions. They need to be evaluated on the desired frequency grid as a post-processing step.

\subsubsection{Gaussian peaks}
The first way to generate random densities is a weighted sum of Gaussian peaks 
\begin{align}
p_0(\omega)
=
\sum_{j=1}^{N} 
\frac{\pi A_j}{\sqrt{2\pi} \sigma_j}
\exp\bigg(-\frac{1}{2}\Big(\frac{\omega-\omega_j}{\sigma_j}\Big)^2\bigg).
\end{align}
Each peak is parametrized by weight $A_j$, center $\omega_j$, and width $\sigma_j$.

Most works using machine learning for analytic continuation start by training on such sets of Gaussian peaks~\cite{Arsenault2017, Yoon2018, Xie2019, Fournier2020}. The most influential paper in that respect is arguably that of Arsenault et al~\cite{Arsenault2017}, which correctly identifies the importance of the dataset as a mean for regulation. 
Their data generation process is described qualitatively, which prevents faithful reproduction. The work of Yoon et al.~\cite{Yoon2018} uses a more complicated distribution for the number of peaks. The dataset defined by Fournier et al.\cite{Fournier2020} and re-used in Xie et al.\cite{Xie2019}, are more straigthforward to generate, but their definition used for the Gaussian implies $A_j=\sigma_j$ which favours very rounded distributions.

\textcolor{red}{Here we consider [...] shown in Fig.~\ref{fig_datasets}(a)}


\subsubsection{Beta peaks}
Another way to generate random distributions, which we introduce here, is a weigthed sum of $\beta$-peaks. This produces much richer function shapes, \textcolor{red}{as shown in Fig.~\ref{fig_datasets}~(b)}. Densities read
\begin{align}
p_0(\omega)
=
\sum_{j=1}^{N} 
\frac{\pi A_j}{\tilde{\sigma}_j}
\text{Beta}\bigg(
\frac{\omega-\tilde{\omega}_j}{\tilde{\sigma}_j}
;\alpha_j,\beta_j
\bigg),
\end{align}
where each peak is parametrized by the amplitude $A_j$, the $\alpha_j$, and the $\beta_j$ of the Beta distribution,
\begin{align}
\text{Beta}(x;\alpha,\beta) 
&=
\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} x^{1-\alpha}(1-x)^{1-\beta},
\end{align}
and, since the latter is only non-zero for $0\leq x\leq 1$, the center $\omega_j$ and the width $\sigma_j$ of the peaks we generate must be adjusted to the mean and standard deviation of the distribution, as
\begin{align}
\tilde{\omega}_j
&=
\omega_j+\sigma_j
\frac{\alpha_j}{\alpha_j+\beta_j}
\\
\tilde{\sigma}_j
&=
\sigma_j
\bigg(
\frac
{(\alpha_j+\beta_j+1)(\alpha_j+\beta_j)^2}
{\alpha_j\beta_j}
\bigg)^{\frac{1}{2}}.
\end{align}
Note that for large $\alpha=\beta$, the Beta distribution becomes very similar to the Gaussian distribution. Values of $\alpha=\beta\gtrsim 10$ are sufficient to make them indistinguishable on a plot. However, the tails of the Gaussian extend to infinity, whereas the tails of the Beta distribution are always bounded within $0 \leq \frac{\omega-\tilde{\omega}_j}{\tilde{\sigma}_j} \geq 1$. We hypothesize that learning from densities with Beta-shaped peaks should easily generalize to those with Gaussian-shaped peaks.

\begin{figure}
\caption{Datasets}
\label{fig_datasets}
\end{figure}

% Note also that for disctrete values of $\alpha$ and $\beta$, the beta distribution is identical to Bernstein polynomials
% \begin{align}
% b_{m,n}(x)
% &=
% \binom{m}{n} x^n(1-x)^{m-n}
% \end{align}
% where $\binom{m}{n}$ is the binomial coefficient, and $m$ and $n$ are integer parameters. They can be related to the real-valued $\alpha$ and $\beta$ of the Beta distribution by
% \begin{align}
% m &= \alpha+\beta-2
% \\
% n &= \alpha-1.
% \end{align}
% The normalization, mean, and standard deviation of these polynomials are respectively given by
% \begin{align}
% a_{m,n}
% &=
% \frac{1}{m+1},
% \\
% \mu_{m,n}
% &=
% \frac{1+n}{2+m},
% \\
% \sigma_{m,n}
% &=
% \bigg(
% \frac{(1+n)(2+n)}{(2+m)(3+m)}
% -
% \frac{(1+n)^2}{(2+m)^2}
% \bigg)^{\frac{1}{2}}.
% \end{align}
% so it is possible to adjust them to control their center and width as we did for the Beta distribution. It is worth noting that Bernstein polynomials are used to write Bezier curves in polynomial form, and they provide a constructive proof of the Weierstrass approximation theorem.



\subsection{Preparing input-ouput pairs}
\label{sec_Pi}
Once $p_0(\omega)$ is generated, we must integrate~\eqref{dimensionless_with_p1} to obtain $\vec \Pi$, and we must sample $p_0(\omega)$ on the frequency grid to obtain the input-output ($\vec \Pi$, $\vec p$) pair. Integration can be performed with off-the-shelf numerical integrators and will not be covered here.
What requires care is how to choose the temperature $T$ at which to obtain $\vec\Pi$ and how to choose the frequency scale $s$ (or equivalently, the grid $s\Delta\omega$) for sampling $\vec p$. To illustrate the importance of these choices, we will compare four strategies, \textcolor{red}{with resulting datasets shown in Fig.~\ref{fig_prep_compare}}. We also discuss how to use the fixed-$s$ scheme to control undesired corrleations between $\vec \Pi$ and $\vec p$.


\subsubsection{Fixing $T$}
The simplest way to obtain ($\vec \Pi$, $\vec p$)-pairs is to work with the original $p_0(\omega)$ at a fixed reference temperature $T_{\text{ref}}$. We shall identify this first strategy as ``fixed-$T$'':
\begin{enumerate}
    \item \label{fixT}
    \begin{itemize}
        \item[$\vec p$:] Sample and normalize $p_0(\omega)$ as $[\vec p]_m = \Delta\omega p_0(\Delta\omega m)$.
        \item[$\vec \Pi$:] Integrate~\eqref{dimensionless_with_p1} using $p_0(\omega)$ and the reference temperature $T_{\text{ref}}$.
    \end{itemize}
\end{enumerate}
Note that generating $(\vec\Pi, p_0(\omega))$-pairs at fixed temperature naturally produce multiple scales~$s$. By this, we mean that multiple $p_0(\omega)=sp(s\omega)$ from the dataset could share the same reference $p(\omega)$ but with different values of~$s$.
Thus, using a single temperature $T_{\text{ref}}$ to generate the dataset is sufficient to cover multiple $T_{\text{ref}}/s$ and enable temperature-agnostic analytic continuation. However, we will see below that working this way introduces uncontrolled correlations in the dataset.


\subsubsection{Fixing $s$}
\label{sec_data_rescaling}
To rescale the density functions $p_0(\omega) = sp(s\omega)$ from the training dataset to get only reference densities $p(\omega)$, we make a choice: all reference $p(\omega)$ will be have the same second central moment~$\mu_2$. Performing the change of variable $\omega \rightarrow s\omega$ in~\eqref{user_s}, we get
\begin{align}
\mu_2&=
s^2 \int d\omega \omega^2 p_0(\omega),
\end{align}
from which we isolate $1/s$ and prepare the reference $p(\omega)$ as
\begin{align}
\label{eq_prep_ref}
p(\omega) 
&=
\frac{1}{s}p_0\big(\frac{1}{s}\omega\big),
&\text{where: }
\frac{1}{s}
=
\sqrt{
\frac{1}{\mu_2}
\int d\omega \omega^2 p_0(\omega).
}
\end{align}
Preparing each density like this ensures the dataset contains only reference $p(\omega)$ as targets. The chosen value for $\mu_2$ becomes a parameter of the analytic continuation solver.

Fixing $\mu_2$ have two additional advantages. First, all reference distributions will have similar widths in the $\omega$ space; there will be no exessively narrow or wide $p(\omega)$, which is useful when working on predefined frequency grid. Second, it simplifies the recovery of conductivity using Eqs.~\eqref{user_s} in which $\mu_2$ appears explicitly.
% And third, when the generation process for $p_0(\omega)$ allows one to control the second central moment of the distribution analytically, the correct scale can be enforced without numerical overhead (we did not take advantage of this in the current work).

Once we have the reference $p(\omega)$, we must still sample it on the frequency grid to obtain the ($\vec \Pi$, $\vec p$)-pairs. There two equivalent ways to proceed; our second and third strategies. We will identify both of them as ``fixed-$s$'', since they produce the same dataset,
\begin{enumerate}[resume]
\item \label{rescaled}
\begin{itemize}
    \item[$\vec p$:] Get the reference distribution $p(\omega) = \tfrac{1}{s}p_0(\tfrac{1}{s}\omega)$ as specified in~\eqref{eq_prep_ref}, then sample and normalize it as $[\vec p]_m = \Delta\omega p(\Delta\omega m)$.
    \item[$\vec \Pi$:] Integrate~\eqref{dimensionless_with_p1} using the reference $p(\omega)$ and some randomly distributed temperature $T$.
\end{itemize}
\item \label{p0rescaled}
\begin{itemize}
    \item[$\vec p$:] Sample and normalize $p_0(\omega)$ on an adjusted frequency grid $[\vec p]_m = \frac{1}{s}\Delta\omega p_0(\frac{1}{s}\Delta\omega m)$, with $1/s$~specified in~\eqref{eq_prep_ref}.

    \item[$\vec \Pi$:] Integrate~\eqref{dimensionless_with_p1} using the original distribution $p_0(\omega)$ and randomly distributed temperature, but adjusted as $T/s$ with $1/s$~specified in~\eqref{eq_prep_ref}.
\end{itemize}
\end{enumerate}
These two approches yield the same $\vec p$ and $\vec \Pi$ because of the degenerancy of section~\ref{section_temperature_agnostic}, namely, approach~\ref{rescaled} uses the distribution $p(\omega)$ at temperature $T$ whereas approach~\ref{p0rescaled} uses the distribution $p_0(\omega)=sp(s\omega)$ at temperature $T/s$.
Note also that, since $s$ is fixed, we need to use randombly distributed $T$ in order to cover multiple values of $T/s$ and enable temperature-agnostic analytic continuation. This is clarified below.

\subsubsection{Controlling correlations}
Compared to the fixed-$T$ approach, working at fixed-$s$ (approaches \ref{rescaled} and \ref{p0rescaled}) provides more control over some subtle correlations between the $\vec \Pi$ and $\vec p$ contained in the dataset. Such control is important, because it is those correlations that the neural network learns to reproduce.

The correlations we refer to arise as follows. When all $\vec \Pi$ are computed at the same temperature, a narrow distribution $p_0(\omega)$ leads to a fast-decreasing $\vec \Pi$, and a wide $p_0(\omega)$ leads to slow-decreasing $\vec \Pi$, as expected from the tail relation~\eqref{Pi_moment}.
In the fixed-$T$ approach, this correlation manifests directly: fast-decreasing $\vec \Pi$ are paired with narrow $\vec p$ (meaning only the first few components of $\vec p$ are non-negligible) while slow-decreasing $\vec \Pi$ are paired with wide $\vec p$ (all components non-negligible).
By contrast, in the fix-$s$ approach, all reference $p(\omega)$ have the same second moment, hence the same width. As a consequence, if a single value of $T$ is used, all $\vec \Pi$ end up with the same decrease rate, as expected from the tail relation~\eqref{Pi_moment}. 
In order for the dataset to cover multiple decrease rates (multiple values of $T/s$), we need an explicit range for~$T$.

The correlation between decrease rates is not a problem in itself---capturing it is actually required to perform analytic continuation. The real problem is that the manner in which $p_0(\omega)$ is prepared can then lead to other indirect correlations.
For example, if narrower $p_0(\omega)$ tend to have simpler structure (fewer peaks), and wide $p_0(\omega)$ tend to have more complex structure (many peaks), then fast-decreasing $\vec \Pi$ get paired with simpler-structured $\vec p$ and slow-decreasing $\vec \Pi$ get paired with more complex-structured $\vec p$.
Such correlations may or may not be desirable, but one must be aware they exists.
With the fixed-$s$ approach, one has full control over these correlations. Indeed, $p_0(\omega)$ is first recast to the reference scale of $sp(s\omega)$ (and thus a single decrease rate of $\vec \pi$), and one can pick the desired value of $T$ (and thus of the decrease rate of $\vec \Pi$) independently from the structure of $p_0(\omega)$.

To illustrate, let us introduce a fourth strategy for generating ($\vec \Pi$, $\vec p$)-pairs, hybrid betwen the fixed-$s$ and fixed-$T$ approaches, which we will identify ``spurious''
\begin{enumerate}[resume]
    \item \label{spurious}
    \begin{itemize}
        \item[$\vec p$:] Sample and normalize $p_0(\omega)$ on the adjusted frequency grid $[\vec p]_m = \frac{1}{s}\Delta\omega p_0(\frac{1}{s}\Delta\omega m)$, with $\frac{1}{s}$~specified in~\eqref{eq_prep_ref}.

        \item[$\vec \Pi$:] Integrate~\eqref{dimensionless_with_p1} using the original distribution $p_0(\omega)$ at temperature $T_{\text{ref}}$.
    \end{itemize}
\end{enumerate}
In this approach, targets~$\vec p$ are all rescaled to the same width (as in the fixed-$s$ approach), so the correlations between the width of $\vec p$ and decreasing rate of $\vec \Pi$ is removed.
However, inputs $\vec \Pi$ are prepared using $p_0(\omega)$ at fixed temperature (as in the fixed-$T$ approach), so the correlation between the width of $p_0(\omega)$ and the decreasing rate of $\vec \Pi$ remains.
Therefore, spurious correlations between decrease rates and structure will remain.

The above considerations clarify the kind of bias which is encoded in the dataset. Such bias is actually the main benefit of using machine-learning for analytic continuation, because it enables an educated form of regularization. The fixed-$s$ scheme detailed throughout the paper provides detailed control of this regularization---one can modulate the values of $T$ according to the complexity of $p_0(\omega)$. We leave that exploration for future work.

\begin{figure}
    \caption{}
    \label{fig_prep_compare}
\end{figure}

\section{Experiments}

\subsection{Dataset details}


\subsection{Training details}
\label{sec_training}


\paragraph*{Neural network} We considered the case where the input and output sizes are respectively $N=128$ and $M=512$. The optimal neural network we found is fully connected, with four hidden layers roughly of dimension $1000$, $1350$, $1700$, and $1700$. We use batchnorm and rectified linear unit (ReLU) activation functions and a Softmax output unit (producing a normalized $\vec p$). Weights are initialized using Xavier initialization.

\paragraph*{Standardized inputs} Any data that enters the neural network is standardized as $(\vec \Pi - \vec\mu)/\vec\sigma$, where $\vec \mu = \mathbb{E}[\vec \Pi]$ and $\vec \sigma = \sqrt{\mathbb{E}[\vec \Pi^2] - \mathbb{E}[\vec \Pi]^2}$ are respectively the average and the standard deviation of the training inputs dataset(element-wise), with expectation values taken over the training set only. The random noise added to $\vec \Pi$ to simulate Quantum Monte-Carlo response functions is renewed at every epoch and it is added before standardization.

\paragraph*{Loss functions} As a training objective, we compared the performance of cross-entropy, 
\begin{align}
    \text{CE}(\vec p,\hat{\vec p}) = \frac{1}{M}\sum_m p_m \log \hat{p}_m
\end{align}
mean-square error, 
\begin{align}
    \text{MSE}(\vec p,\hat{\vec p}) = \frac{1}{M}\sum_m \sqrt{(p_m - \hat{p}_m)^2}
\end{align}
and mean-absolute error
\begin{align}
    \text{MAE}(\vec p,\hat{\vec p}) = \frac{1}{M}\sum_m |p_m - \hat{p}_m|.
\end{align}
We found that using the mean-absolute error as the training objective leads to better performance.

\paragraph*{Optimizer} We use the Adam optimizer with learning rate of $\alpha = 8\times10^{-5}$ and a scheduler decreasing the rate as $\alpha \rightarrow 0.216\alpha$ whenever the validation loss doesn't improve for five straight epochs. That decrase stops when the learning rate reach $\alpha\sim10^{-10}$. We also use learning rate warm-up, linearly increasing $\alpha$ from zero to its value of $\alpha = 8\times10^{-5}$ during the first epoch.

\paragraph*{Regularization} Our random search over hyper-parameters revealed that neither L2 regularization, dropout improve results. However, using slightly larger input noise at training time than at test time leads to better performance, and training on more complex data (Beta peaks) can lead to better performance on simple data (Gaussian peaks) than training on simple data.

\subsection{Comparing the fixed-$T$ and fixed-$s$ schemes}


\section{Discussion \& Conclusion}


\begin{acknowledgments}
This project was started in close collaboration with Reza Nourafkan and André-Marie Tremblay. I must also acknowledge additional useful discussions with Yoshua Bengio, Maxime Charlebois, Samuel Desrosiers, Sébastien Lachapelle, Vincent Mai and Francesco Rotella.
\end{acknowledgments}

\bibliography{report}
    
\pagebreak

\appendix
\addcontentsline{toc}{section}{Appendix} % Add the appendix text to the document TOC


\section{Refreshers on fundamentals}

\subsection{Complex analysis}

\paragraph*{Residues} The \emph{residue theorem} expresses a contour integral in the complex plane as the sum of residues of the poles (sigularities) in the region enclosed by the counter-clockwise contour
\begin{align}
\oint dz f(z) 
&= 
2\pi i \sum_{k}\text{Res}[f(z),z_k]
\label{residue}
\\
&=
2\pi i \sum_{k}
\frac{1}{(m_k+1)!}
\lim_{z\rightarrow z_k}
\frac{d^{{m_k-1}}}{dz^{m_k-1}}
\Big[(z-z_k)f(z)\Big]
\label{residue_order_m}
&&\text{for poles of order $m_k$}
\\
&=
2\pi i \sum_{k}
\lim_{z\rightarrow z_k}
(z-z_k)f(z)
\label{residue_order_1}
&&\text{for poles of order 1}
\end{align}
As an example, it allows to evaluate following integral
\begin{align}
\oint dz \frac{f(z)}{z-z_0} = 2\pi i f(z_{0}).
\label{simplest_residue}
\end{align}

\paragraph*{Causality} When considering a spectrum $f(z)$ (Fourier transform of $f(t)$) which is analytical in the upper-half complex frequency plane, except at certain poles, and for which $\lim_{|z|\rightarrow\infty} f(z) = 0$, the \emph{lemme of Jordan} allows us to complete the Fourier integral for all $t<0$ with a non-contributing half-circle contour. This allow to turn the integral into the sum of residues
\begin{align}
f(t<0)
&=
\int_{-\infty}^{\infty} dz e^{-iz t} f(z)
+ \underbrace{
\int_{\curvearrowleft} dz e^{-iz t}f(z)
}_{=0}
\label{causal1}
\\&= 
2\pi i \sum_{k}\text{Res}[f(z),z_k].
\label{causal2}
\end{align}
As a consequence, a pole-free spectrum $f(z)$ in the upper-half complex plane ($z=z'+iz''$ with $z''>0$ such that $e^{-z''t}$ ensures convergence) correspond to a causal function in time ($f(t<0)=0$).

\paragraph*{Sokhotski–Plemelj theorem} Finally, we remind the useful theorem
\begin{align}
\lim_{\eta\rightarrow 0}\int_{\-\infty}^{\infty} \frac{f(z)}{z \pm i\eta} dz
=  
\mathcal P \int_{\-\infty}^{\infty} \frac{f(z)}{z} dz \mp \pi i f(z).
\label{Sokhotski_Plemelj}
\end{align}


\subsection{Linear response theory}
In classical systems, a ``linear response function'' $\chi(\omega)$ relates a driving force $F(t)$ to the solution $x(t)$ of a given differential equation. It is defined from a linear relation in the frequency domain as
\begin{align}
x(\omega)
&= \chi(\omega) F(\omega).
\label{eq_response_freq_def}
\end{align}
In the time domain, because of the the convolution theorem, this definition becomes
\begin{align} 
x(t) = \int_{-\infty}^{\infty}dt' \chi(t-t') F(t').
\label{eq_response_def}
\end{align}
In particular, any differential equations of the form
\begin{align}
\sum_{n=0}^{\infty} a_n \frac{\partial^n x(t)}{\partial t^n}
&= 
\sum_{n=0}^{\infty} b_m \frac{\partial^m F(t)}{\partial t^m},
\end{align}
can be Fourier transformed such that the linear response is the exact response
\begin{align}
x(\omega) &= \underbrace{\frac{\sum_{m} b_m (-i\omega)^m}{\sum_{n} a_n (-i\omega)^n}}_{\chi(\omega)} F(\omega),
\end{align}

In quantum systems, response functions show up as $\chi^{R}_{A,B}(\omega)$ in time-dependent perturbation theory. Considering the perturbation $\delta H(t) = a(t) A(t)$, proportional to observable $A(t)$ (hence the subscript $A$ in $\chi^{R}_{A,B}(\omega)$), and its effect on the expectation value of another observable $\langle B(t) \rangle$ (hence the subscript $B$ in $\chi^{R}_{A,B}(\omega)$), the linear approximation for time evolution in the interaction picture (not shown here) leads to an expression equivalent to~\eqref{eq_response_def},
\begin{align}
    \langle B(t) \rangle - \langle B(t) \rangle_0 
    &=
    \frac{i}{\hbar} 
    \int_{-\infty}^{t} dt'
    \langle [B(t), \delta H(t')] \rangle
    \\&=
    \int_{-\infty}^{\infty}dt'
    \underbrace{ 
    \frac{i}{\hbar} 
    \langle [B(t), A(t')] \rangle 
    \theta(t-t')
    }_{\chi^{R}_{A,B}(t-t')}
    a(t').
\end{align}
Note that the response is said to be \emph{causal}, or \emph{retarded} (hence the $R$ superscript) because the Heaviside step function $\theta(t-t')$ ensures the response is zero if $t<t'$. This lead to a pole-free spectrum in the upper-half complex plane (as discussed at equations \eqref{causal1}-\eqref{causal2}).


\subsection{Spectral representation}
Taking the Fourier transform of the above $\chi^R_{A,B}(t-t')$ leads to the spectral representation.
Omitting the $A,B$ subscript for simplicity, we can rewrite it as
\begin{align}
\chi^{R}(t-t') = 2i\chi''(t-t')\theta(t-t'),
\label{chi_retarded}
\end{align}
with
\begin{align}
\chi''(t-t') = \frac{1}{2\hbar}\langle [A_I(t),B_I(t')]\rangle.   
\end{align}
The Fourier transform of \eqref{chi_retarded} is then
\begin{align}
\chi(z) = \int_{-\infty}^{\infty} \frac{d\omega}{\pi} \frac{\chi''(\omega)}{\omega-z}
\label{spectral_representation}
\end{align}
where $\chi''(\omega)$ is the Fourier transform of $\chi''(t-t')$, and is real-valued if the latter is even. Since $\chi^{R}(t-t')$ is causal, $\chi(z)$ is analytic in the upper half plane of the complex frequency $z$ (as discussed at equations \eqref{causal1}-\eqref{causal2}). The above $\chi(z)$ unifies various definitions of the response function, notably the advanced response function $\chi^{A}(\omega) = \chi(\omega-i\eta)$, the retarded response functions $\chi^{R}(\omega) = \chi(\omega+i\eta)$, and most important, the the Matsubara response functions $\chi(i\omega_n)$. The latter is
sampled at discrete Matsubara frequencies $\omega_{n} = \frac{2\pi}{\beta}n$ with $n$ integer and the inverse temperature $\beta=\frac{1}{k_B T}$.

Finally, our definitions ensure that $\chi''(\omega)$ is the imaginary part of $\chi^R(\omega)$, as seen with~\eqref{Sokhotski_Plemelj}
\begin{align}
\chi^R(\omega) = \chi(\omega+i\eta) 
&= 
\int_{-\infty}^{\infty} \frac{d\omega'}{\pi} \frac{\chi''(\omega')}{\omega'-\omega-i\eta}
\\
&=
\mathcal P\int_{-\infty}^{\infty} \frac{d\omega'}{\pi} \frac{\chi''(\omega')}{\omega'-\omega}
+i\chi''(\omega)
\label{kramers_plus_imaginary}
\\
&=\chi'(\omega)+i\chi''(\omega).
\end{align}
The first term of \eqref{kramers_plus_imaginary} is one of the two \emph{Kramer-Kronig relations} (\emph{Hilbert transforms}):
\begin{align}
\chi'(\omega) &= \mathcal P\int_{-\infty}^{\infty} \frac{d\omega'}{\pi} \frac{\chi''(\omega')}{\omega'-\omega}
\\
\chi''(\omega) &= -\mathcal P\int_{-\infty}^{\infty} \frac{d\omega'}{\pi} \frac{\chi'(\omega')}{\omega'-\omega}.
\end{align}

\subsection{Optical conductivity}

One particularly important response function is the current-current correlation function $\Pi^R(\omega) = \chi_{j_x,j_x}^R(\omega)$, which is related to the optical conductivity $\sigma(\omega)$ by the \emph{Kubo formula}
\begin{align}
\sigma(\omega)
&=
\frac{1}{i(\omega + i\eta)}\bigg[\Pi^R(\omega)-\frac{ne^2}{m}\bigg].
\label{used_sum_rule1}
\end{align}
From there, we can develop by substituting the spectral representation of $\Pi^{R}(\omega)$,
\begin{align}
\sigma(\omega)&=
\frac{1}{i(\omega + i\eta)}
\bigg[
\int_{-\infty}^{\infty}d\omega'\frac{\Pi''(\omega')}{\omega'-(\omega+i\eta)}
-\int_{-\infty}^{\infty}d\omega'\frac{\Pi''(\omega')}{\omega'}
\bigg]
\label{used_sum_rule2}
\\
&=
\frac{1}{i(\omega + i\eta)}
\int_{-\infty}^{\infty}d\omega'
\Pi''(\omega') 
\frac{\omega'-(\omega'-(\omega+i\eta))}{\omega'(\omega'-(\omega+i\eta))}
\\
&=
\int_{-\infty}^{\infty}d\omega'\frac{\Pi''(\omega')/i\omega'}{\omega'-(\omega+i\eta)}
\\
&=
\frac{\Pi''(\omega')}{\omega'}
-i\mathcal P \int_{-\infty}^{\infty}d\omega'\frac{\Pi''(\omega')/\omega'}{\omega'-\omega}.
\label{conductivity_spectral}
\end{align}
The last terms in lines \eqref{used_sum_rule1} and \eqref{used_sum_rule2} are equal as a result of the \emph{f-sum rule}. The last line was again obtained with~\eqref{Sokhotski_Plemelj} and allows us to extract the important relation
\begin{align}
\text{Re}\{ \sigma(\omega) \} = \frac{\Pi''(\omega)}{\omega}.
\end{align}
For Matsubara frequencies $z=i\omega_n$, the spectral representation $\Pi(z)$ then simplifies to
\begin{align}
\Pi(i\omega_n) 
&=
\int_{-\infty}^{\infty} \frac{d\omega}{\pi}\frac{\omega}{\omega-i\omega_n}\text{Re}\{ \sigma(\omega) \}
\label{chkpt_Pi_continuation_1}
\\
&=
\int_{-\infty}^{\infty} \frac{d\omega}{\pi} \frac{\omega^2}{\omega^2+\omega_n^2}
\text{Re}\{ \sigma(\omega) \}
+
\underbrace{i
\int_{-\infty}^{\infty} \frac{d\omega}{\pi} \frac{\omega\omega_n}{\omega^2+\omega_n^2}
\text{Re}\{ \sigma(\omega) \}
}_{=0}.
\label{chkpt_Pi_continuation}
\end{align}
The last term vanishes because $\text{Re}\{\sigma(\omega)\}$ is even (not shown), which guarantees that $\Pi(i\omega_n)$ is real. Equation~\eqref{chkpt_Pi_continuation} is the same as~\eqref{Pi_continuation}.

\section{Miscellaneous}
\subsection{Alternative retrieval of $\Delta\omega$}

Once the user obtain the estimate $\hat{\vec p}$ with the neural network, the only thing left to do is to find the frequency grid $\Delta\omega$ on which to interpret that estimate. Considering the discretization~\eqref{discretized_continuation_1} for the special case of the tail relation~\eqref{Pi_moment},
\begin{align}
    2\pi k_B T \lim_{n\rightarrow \infty}n^2 \Pi_n = \Delta\omega^2 \sum_m m^2 p_m
\end{align}
We see that $\vec \Pi$ and $\vec p$ provide a closed relation between the temperature and the frequency spacing. If we assume that the discretization is a good approximation for the integral and that the last component $\Pi_N$ correctly captures the limit $n\rightarrow\infty$, then we can cast this relation as
\begin{align}
    \Delta\omega^2 = 
    \frac{
        2\pi k_B 
        % \lim_{n\rightarrow \infty}n^2 \Pi_n
        N^2 \Pi_N
    }{
        \sum_m m^2 p_m
    }
    T.
\label{eq_minimal_user}
\end{align}
This is a discrete version of~\eqref{user_s} which can be used to recover $\Delta\omega$ from the user's temperature $T$. In principle, however, the continuous version should be preferred. A simple way to improve on~\eqref{eq_minimal_user} is to use a better estimate for $\lim_{\omega_n\rightarrow \infty}\omega_n^2 \Pi_n$, than $2\pi k_B T N^2\Pi_N$. Note that if fixed $T_{\text{ref}}$ was used in training, this task is considerably simplified, as one can use $T/T_{\text{ref}}$


\subsection{Lorentz combs with analytic integration}
An alternative way to generate a random distributions is to express $\Pi''(\omega)$ as a sum of Dirac delta functions approximated by Lorentzians (Cauchy distributions)
\begin{align}
\text{Re}\{ \sigma(\omega) \} = \frac{\Pi''(\omega)}{\omega}
% =
% \frac{\Pi''(\omega)}
% {\omega}
&=
\frac{\pi}{\omega}
\sum_{j=i}^{J}
\Big[
a_j\delta(\omega-\omega_j)
-a_j\delta(\omega+\omega_j)
\Big]
&&\text{with }\omega_j\geq0
\label{Pi_as_delta}
\\
&\approx
\sum_{j=i}^{J}
\frac{\pi a_j}{\omega}
\Big[
\frac{1}{\pi}
\frac{\eta}{(\omega-\omega_j)^2+\eta^2}
-
\frac{1}{\pi}
\frac{\eta}{(\omega+\omega_j)^2+\eta^2}
\Big]
\label{Pi_as_lorentzians}
\\
&\approx
\sum_{j=i}^{J}
\frac{a_j\eta}{\omega}
\frac{
\big((\omega+\omega_j)^2+\eta^2\big) 
-\big((\omega-\omega_j)^2+\eta^2\big)
}{
\big((\omega-\omega_j)^2+\eta^2\big)
\big((\omega+\omega_j)^2+\eta^2\big)
}
\\
&\approx
\sum_{j=i}^{J}
\frac{a_j\eta}{\omega}
\frac{
4\omega\omega_j
}{
(\omega^2+\omega_j^2+\eta^2)^2 - (2\omega\omega_j)^2
}.
\label{sigma_lorentz}
\end{align}
This form has the advantage of removing the integral when computing $\Pi(i\omega_n)$, as hinted by the Dirac delta.
% To show this, we substitute~\eqref{Pi_as_delta} into~\eqref{chkpt_Pi_continuation_1}
% \begin{align}
% \Pi(i\omega_n)
% &=
% \int_{-\infty}^{\infty} \frac{d\omega}{\pi}\frac{\omega}{\omega-i\omega_n}
% \frac{\pi}{\omega}
% \sum_{j=i}^{J}
% \Big[
% a_j\delta(\omega-\omega_j)
% -a_j\delta(\omega+\omega_j)
% \Big]
% &&\text{with }\omega_j\geq0
% \\
% % &=
% % \sum_{j=i}^{J}
% % \Big[
% % \frac{a_j}{\omega_j-i\omega_n}
% % -\frac{a_j}{-\omega_j-i\omega_n}
% % \Big]
% % \\
% &=
% \sum_{j=i}^{J}
% \frac{2a_j\omega_j}{\omega_j^2+\omega_n^2}.
% \label{exact_Pi_from_delta}
% \end{align}
% In the numerical case however, 
We can even track the finite width $\eta$ by substituting~\eqref{Pi_as_lorentzians} into~\eqref{chkpt_Pi_continuation_1}
\begin{align}
\Pi(i\omega_n)
&\approx
\sum_{j=i}^{J}
a_j
\int_{-\infty}^{\infty} \frac{d\omega}{\pi}
\frac{1}{\omega-i\omega_n}
\Big[
\frac{\eta}{(\omega-\omega_j)^2+\eta^2}
-
\frac{\eta}{(\omega+\omega_j)^2+\eta^2}
\Big],
\label{Pi_with_eta}
\end{align}
where the residue theorem~\eqref{residue} let us solve the two integrals using the contours shown in Fig.~\ref{contour}
\begin{align}
I_{\pm}
&=
\int_{-\infty}^{\infty} \frac{d\omega}{\pi}
\frac{1}{\omega-i\omega_n}
\frac{\eta}{(\omega\pm\omega_j)^2+\eta^2}
\\
&=
\frac{\eta}{\pi}
\int_{-\infty}^{\infty} d\omega
\frac{1}{\omega-i\omega_n}
\frac{1}{
(\omega\pm\omega_j-i\eta)
(\omega\pm\omega_j+i\eta)
}
\label{integral_I}
\\
&=
\frac{\eta}{\pi}
\bigg[
-2\pi i
\frac{1}{(\mp\omega_j-i\eta-i\omega_n)(-2i\eta)}
\bigg]
% &&\text{residue for pole }\omega_{\times}=\mp\omega_j-i\eta
\\
&=
\frac{
\mp \omega_j + i\eta + i\omega_n
}{
\omega_j^2+(\omega_n+\eta)^2
},
\end{align}
\begin{figure}
\center
\includegraphics{contour.pdf}
\caption{Contours for integral $I_{\pm}$~\eqref{integral_I}. The $C_{\infty}$ arcs do not contribute thanks to Jordan's lemma.}
\label{contour}
\end{figure}
simplifying~\eqref{Pi_with_eta} to
% (which is consistent with~\eqref{exact_Pi_from_delta} for $\eta\rightarrow0$)
\begin{align}
\Pi(i\omega_n)
\approx
\sum_{j=i}^{J}
\frac{2a_j\omega_j}{\omega_j^2+(\omega_n+\eta)^2}.
\label{Pi_lorentz}
\end{align}

Therefore,~\eqref{sigma_lorentz} and~\eqref{Pi_lorentz} can be used to generate $p(\omega)$ and $\Pi(i\omega_n)$ data without having to perform costly numerical integrals. We can further get arbitrary $p(\omega)$ either by modulating the peak amplitudes $a_j$, or we can distribute their centers $\omega_j$ non-uniformly. The resulting function will be smooth as long as the width of the peaks are at least twice as large as the spacing between them, and the moments of the spectrum will be well-defined if a large enough number $J$ of Lorentzians are summed. Unfortunately, in practice, this sum can become as costly as a numerical integral.

\end{document}

