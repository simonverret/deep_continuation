Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Arsenault2017,
abstract = {We present a supervised machine learning approach to the inversion of Fredholm integrals of the first kind as they arise, for example, in the analytic continuation problem of quantum many-body physics. The approach provides a natural regularization for the ill-conditioned inverse of the Fredholm kernel, as well as an efficient and stable treatment of constraints. The key observation is that the stability of the forward problem permits the construction of a large database of outputs for physically meaningful inputs. Applying machine learning to this database generates a regression function of controlled complexity, which returns approximate solutions for previously unseen inputs; the approximate solutions are then projected onto the subspace of functions satisfying relevant constraints. Under standard error metrics the method performs as well or better than the Maximum Entropy method for low input noise and is substantially more robust to increased input noise. We suggest that the methodology will be similarly effective for other problems involving a formally ill-conditioned inversion of an integral operator, provided that the forward problem can be efficiently solved.},
archivePrefix = {arXiv},
arxivId = {1612.04895},
author = {Arsenault, Louis Fran{\c{c}}ois and Neuberg, Richard and Hannah, Lauren A. and Millis, Andrew J.},
doi = {10.1088/1361-6420/aa8d93},
eprint = {1612.04895},
file = {:Users/Simon/Documents/papiers{\_}mendeley//2017 - Arsenault et al. - Projected regression method for solving Fredholm integral equations arising in the analytic continuation probl.pdf:pdf;:Users/Simon/Documents/papiers{\_}mendeley/2017 - Arsenault et al. - Projected regression method for solving Fredholm integral equations arising in the analytic continuation probl.pdf:pdf},
issn = {13616420},
journal = {Inverse Probl.},
keywords = {analytical continuation,integral equations,machine learning,regularization},
number = {11},
pages = {1--9},
publisher = {IOP Publishing},
title = {{Projected regression method for solving Fredholm integral equations arising in the analytic continuation problem of quantum physics}},
url = {http://arxiv.org/abs/1612.04895 https://iopscience.iop.org/article/10.1088/1361-6420/aa8d93/meta},
volume = {33},
year = {2017}
}
@article{Brown2019a,
abstract = {De novo design seeks to generate molecules with required property profiles by virtual design-make-test cycles. With the emergence of deep learning and neural generative models in many application areas, models for molecular design based on neural networks appeared recently and show promising results. However, the new models have not been profiled on consistent tasks, and comparative studies to well-established algorithms have only seldom been performed. To standardize the assessment of both classical and neural models for de novo molecular design, we propose an evaluation framework, GuacaMol, based on a suite of standardized benchmarks. The benchmark tasks encompass measuring the fidelity of the models to reproduce the property distribution of the training sets, the ability to generate novel molecules, the exploration and exploitation of chemical space, and a variety of single and multiobjective optimization tasks. The benchmarking open-source Python code and a leaderboard can be found on https://benevolent.ai/guacamol.},
archivePrefix = {arXiv},
arxivId = {1811.09621},
author = {Brown, Nathan and Fiscato, Marco and Segler, Marwin H.S. and Vaucher, Alain C.},
doi = {10.1021/acs.jcim.8b00839},
eprint = {1811.09621},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2019 - Brown et al. - GuacaMol Benchmarking Models for de Novo Molecular Design.pdf:pdf},
issn = {15205142},
journal = {J. Chem. Inf. Model.},
month = {mar},
number = {3},
pages = {1096--1108},
pmid = {30887799},
publisher = {American Chemical Society},
title = {{GuacaMol: Benchmarking Models for de Novo Molecular Design}},
volume = {59},
year = {2019}
}
@article{Jarrell1996,
abstract = {We present a way to use Bayesian statistical inference and the principle of maximum entropy to analytically continue imaginary-time quantum Monte Carlo data. We supply the details that are lacking in the seminal literature but are important for the motivated reader to understand the assumptions and approximations embodied in these methods. First, we summarize the general relations between quantum correlation functions and spectral densities. We then review the basic principles, formalism, and philosophy of Bayesian inference and discuss the application of this approach in the context of the analytic continuation problem. Next, we present a detailed case study for the symmetric, infinite-dimension Anderson Hamiltonian. We chose this Hamiltonian because the qualitative features of its spectral density are well established and because a particularly convenient algorithm exists to produce the imaginary-time Green's function data. Shown are all the intermediate steps of data and solution qualification. The importance of careful data preparation and error propagation in the analytic continuation is discussed in the context of this example. Then, we review the different physical systems and physical quantities to which these, or related, procedures have been applied. Finally, we describe other features concerning the application of our methods, their possible improvement, and areas for additional study.},
author = {Jarrell, Mark and Gubernatis, J. E.},
doi = {10.1016/0370-1573(95)00074-7},
file = {:Users/Simon/Documents/papiers{\_}mendeley/1996 - Jarrell, Gubernatis - Bayesian inference and the analytic continuation of imaginary-time quantum Monte Carlo data.pdf:pdf},
issn = {03701573},
journal = {Phys. Rep.},
number = {3},
pages = {133--195},
title = {{Bayesian inference and the analytic continuation of imaginary-time quantum Monte Carlo data}},
volume = {269},
year = {1996}
}
@article{Verret2017a,
abstract = {In scanning tunneling microscopy conductance curves, the superconducting gap of cuprates is sometimes accompanied by small subgap structures at very low energy. This was documented early on near vortex cores and later at zero magnetic field. Using mean-field toy models of coexisting d-wave superconductivity, d-form-factor density wave, and extended s-wave pair density wave (s′PDW), we find agreement with this phenomenon, with s′PDW playing a critical role. We explore the high variability of the gap structure with changes in band structure and density wave (DW) wave vector, thus explaining why subgap structures may not be a universal feature in cuprates. In the absence of nesting, nonsuperconducting results never show signs of pseudogap, even for large density wave magnitudes, therefore reinforcing the idea of a distinct origin for the pseudogap, beyond mean-field theory. Therefore, we also briefly consider the effect of DWs on a preexisting pseudogap.},
author = {Verret, S. and Charlebois, M. and S{\'{e}}n{\'{e}}chal, D. and Tremblay, Andre Marie S.},
doi = {10.1103/PhysRevB.95.054518},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2017 - Verret et al. - Subgap structures and pseudogap in cuprate superconductors Role of density waves.pdf:pdf},
issn = {24699969},
journal = {Phys. Rev. B},
number = {5},
pages = {1--12},
title = {{Subgap structures and pseudogap in cuprate superconductors: Role of density waves}},
volume = {95},
year = {2017}
}
@article{Yang2020,
author = {Yang, Tristan},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2020 - Yang - D Eep D Ouble D Escent.pdf:pdf},
number = {1},
pages = {1--24},
title = {{D Eep D Ouble D Escent :}},
year = {2020}
}
@article{Acharya1989,
author = {Acharya, R. and {Narayana Swamy}, P.},
doi = {10.1007/BF02800336},
file = {:Users/Simon/Documents/papiers{\_}mendeley/1989 - Acharya, Narayana Swamy - Температу.pdf:pdf},
issn = {0369-3546},
journal = {Nuovo Cim. A},
number = {5},
pages = {1255--1266},
title = {{Температура выхода кварковT c и температура кирального восстановленияT c в непрерывной квантовой хромодинамике}},
volume = {102},
year = {1989}
}
@article{Dyson2010,
abstract = {The author calls mathematicians who take a lofty c; by examples from the past and his personal acquain; how both types have advanced mathematics.},
author = {Dyson, F.},
doi = {10.3367/ufnr.0180.201008f.0859},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2010 - Dyson - Birds and frogs in mathematics and physics.pdf:pdf},
issn = {0042-1294},
journal = {Uspekhi Fiz. Nauk},
number = {8},
pages = {859},
title = {{Birds and frogs in mathematics and physics}},
volume = {180},
year = {2010}
}
@article{Bradley2020a,
abstract = {Classical probability distributions on sets of sequences can be modeled using quantum states. Here, we do so with a quantum state that is pure and entangled. Because it is entangled, the reduced densities that describe subsystems also carry information about the complementary subsystem. This is in contrast to the classical marginal distributions on a subsystem in which information about the complementary system has been integrated out and lost. A training algorithm based on the density matrix renormalization group (DMRG) procedure uses the extra information contained in the reduced densities and organizes it into a tensor network model. An understanding of the extra information contained in the reduced densities allow us to examine the mechanics of this DMRG algorithm and study the generalization error of the resulting model. As an illustration, we work with the even-parity dataset and produce an estimate for the generalization error as a function of the fraction of the dataset used in training.},
archivePrefix = {arXiv},
arxivId = {arXiv:1910.07425v1},
author = {Bradley, Tai-Danae and Stoudenmire, E M and Terilla, John},
doi = {10.1088/2632-2153/ab8731},
eprint = {arXiv:1910.07425v1},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2020 - Bradley, Stoudenmire, Terilla - Modeling sequences with quantum states a look under the hood.pdf:pdf},
issn = {2632-2153},
journal = {Mach. Learn. Sci. Technol.},
month = {jul},
number = {3},
pages = {035008},
title = {{Modeling sequences with quantum states: a look under the hood}},
url = {https://iopscience.iop.org/article/10.1088/2632-2153/ab8731},
volume = {1},
year = {2020}
}
@article{Campos2019,
abstract = {We compute the partition function of a massive free boson in a square lattice using a tensor network algorithm. We introduce a singular value decomposition of continuous matrices that leads to very accurate numerical results. It shows the emergence of a corner double line fixed-point structure. In the massless limit, we reproduce the results of conformal field theory including a precise value of the central charge.},
annote = {first paper generalizing tensor networs to field using Hubbard Stratonovich transformation},
archivePrefix = {arXiv},
arxivId = {1902.02362},
author = {Campos, Manuel and Sierra, Germ{\'{a}}n and L{\'{o}}pez, Esperanza},
doi = {10.1103/PhysRevB.100.195106},
eprint = {1902.02362},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2019 - Campos, Sierra, L{\'{o}}pez - Tensor renormalization group in bosonic field theory.pdf:pdf},
issn = {24699969},
journal = {Phys. Rev. B},
keywords = {doi:10.1103/PhysRevB.100.195106 url:https://doi.or},
number = {19},
pages = {1--11},
publisher = {American Physical Society},
title = {{Tensor renormalization group in bosonic field theory}},
volume = {100},
year = {2019}
}
@article{Korablyov2019,
author = {Korablyov, Maksym and Goyal, Anirudh and Chiu, Erica and Bronstein, Michael and Bengio, Yoshua},
file = {:Users/Simon/Documents/papiers{\_}mendeley//2019 - Korablyov et al. - Fragment-based drug design with Small Prior Neural Networks.pdf:pdf},
number = {NeurIPS},
title = {{Fragment-based drug design with Small Prior Neural Networks}},
year = {2019}
}
@article{Kolda2009,
author = {Kolda, Tamara G and Bader, Brett W},
doi = {10.1137/07070111X},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2009 - Kolda, Bader - Tensor Review.pdf:pdf},
issn = {0036-1445},
journal = {SIAM Rev.},
keywords = {15a69,65f99,ams subject classifications,candecomp,canonical decomposition,higher-order principal components analysis,higher-order singular value decomposition,hosvd,multilinear algebra,multiway arrays,parafac,parallel factors,tensor decompositions,tucker},
number = {3},
pages = {455--500},
title = {{Tensor Review}},
url = {http://link.aip.org/link/?SIR/51/455{\%}5Cnhttp://epubs.siam.org/doi/abs/10.1137/07070111X{\%}5Cnpapers3://publication/uuid/2CAD3CE8-5FD4-4EFB-87B5-E8EA5C3889C0},
volume = {51},
year = {2009}
}
@article{Kanwar2020,
abstract = {We define a class of machine-learned flow-based sampling algorithms for lattice gauge theories that are gauge-invariant by construction. We demonstrate the application of this framework to U(1) gauge theory in two spacetime dimensions, and find that near critical points in parameter space the approach is orders of magnitude more efficient at sampling topological quantities than more traditional sampling procedures such as Hybrid Monte Carlo and Heat Bath.},
archivePrefix = {arXiv},
arxivId = {2003.06413},
author = {Kanwar, Gurtej and Albergo, Michael S. and Boyda, Denis and Cranmer, Kyle and Hackett, Daniel C. and Racani{\`{e}}re, S{\'{e}}bastien and Rezende, Danilo Jimenez and Shanahan, Phiala E.},
eprint = {2003.06413},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2020 - Kanwar et al. - Equivariant flow-based sampling for lattice gauge theory.pdf:pdf},
pages = {1--6},
title = {{Equivariant flow-based sampling for lattice gauge theory}},
url = {http://arxiv.org/abs/2003.06413},
year = {2020}
}
@article{Nielsen2020,
abstract = {We define a class of tensor network states for spin systems where the individual tensors are functionals of fields. The construction is based on the path integral representation of correlators of operators in quantum field theory. These tensor network states are infinite dimensional versions of matrix product states and projected entangled pair states. We find the field-tensor that generates the Haldane-Shastry wave function and extend it to two dimensions. We give evidence that the latter underlies the topological chiral state described by the Kalmeyer-Laughlin wave function.},
annote = {Second paper generalizing tensor network methods to field},
archivePrefix = {arXiv},
arxivId = {2001.07723},
author = {Nielsen, Anne E. B. and Herwerth, Benedikt and Cirac, J. Ignacio and Sierra, Germ{\'{a}}n},
eprint = {2001.07723},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2020 - Nielsen et al. - Field Tensor Network States.pdf:pdf},
title = {{Field Tensor Network States}},
url = {http://arxiv.org/abs/2001.07723},
year = {2020}
}
@article{Fuchs2020,
abstract = {We introduce the SE(3)-Transformer, a variant of the self-attention module for 3D point clouds, which is equivariant under continuous 3D roto-translations. Equivariance is important to ensure stable and predictable performance in the presence of nuisance transformations of the data input. A positive corollary of equivariance is increased weight-tying within the model, leading to fewer trainable parameters and thus decreased sample complexity (i.e. we need less training data). The SE(3)-Transformer leverages the benefits of self-attention to operate on large point clouds with varying number of points, while guaranteeing SE(3)-equivariance for robustness. We evaluate our model on a toy {\$}N{\$}-body particle simulation dataset, showcasing the robustness of the predictions under rotations of the input. We further achieve competitive performance on two real-world datasets, ScanObjectNN and QM9. In all cases, our model outperforms a strong, non-equivariant attention baseline and an equivariant model without attention.},
archivePrefix = {arXiv},
arxivId = {2006.10503},
author = {Fuchs, Fabian B. and Worrall, Daniel E. and Fischer, Volker and Welling, Max},
eprint = {2006.10503},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2020 - Fuchs et al. - SE(3)-Transformers 3D Roto-Translation Equivariant Attention Networks.pdf:pdf},
number = {3},
title = {{SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks}},
url = {http://arxiv.org/abs/2006.10503},
year = {2020}
}
@article{Stokes2020a,
abstract = {Due to the rapid emergence of antibiotic-resistant bacteria, there is a growing need to discover new antibiotics. To address this challenge, we trained a deep neural network capable of predicting molecules with antibacterial activity. We performed predictions on multiple chemical libraries and discovered a molecule from the Drug Repurposing Hub—halicin—that is structurally divergent from conventional antibiotics and displays bactericidal activity against a wide phylogenetic spectrum of pathogens including Mycobacterium tuberculosis and carbapenem-resistant Enterobacteriaceae. Halicin also effectively treated Clostridioides difficile and pan-resistant Acinetobacter baumannii infections in murine models. Additionally, from a discrete set of 23 empirically tested predictions from {\textgreater}107 million molecules curated from the ZINC15 database, our model identified eight antibacterial compounds that are structurally distant from known antibiotics. This work highlights the utility of deep learning approaches to expand our antibiotic arsenal through the discovery of structurally distinct antibacterial molecules. A trained deep neural network predicts antibiotic activity in molecules that are structurally different from known antibiotics, among which Halicin exhibits efficacy against broad-spectrum bacterial infections in mice.},
annote = {Used chemprop to discover new antibiotics},
author = {Stokes, Jonathan M. and Yang, Kevin and Swanson, Kyle and Jin, Wengong and Cubillos-Ruiz, Andres and Donghia, Nina M. and MacNair, Craig R. and French, Shawn and Carfrae, Lindsey A. and Bloom-Ackerman, Zohar and Tran, Victoria M. and Chiappino-Pepe, Anush and Badran, Ahmed H. and Andrews, Ian W. and Chory, Emma J. and Church, George M. and Brown, Eric D. and Jaakkola, Tommi S. and Barzilay, Regina and Collins, James J.},
doi = {10.1016/j.cell.2020.01.021},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2020 - Stokes et al. - A Deep Learning Approach to Antibiotic Discovery.pdf:pdf},
issn = {10974172},
journal = {Cell},
keywords = {antibiotic resistance,antibiotic tolerance,antibiotics,drug discovery,machine learning},
number = {4},
pages = {688--702.e13},
publisher = {Elsevier Inc.},
title = {{A Deep Learning Approach to Antibiotic Discovery}},
url = {https://doi.org/10.1016/j.cell.2020.01.021},
volume = {180},
year = {2020}
}
@article{Tilloy2019,
abstract = {We introduce a new class of states for bosonic quantum fields which extend tensor network states to the continuum and generalize continuous matrix product states to spatial dimensions d≥2. By construction, they are Euclidean invariant and are genuine continuum limits of discrete tensor network states. Admitting both a functional integral and an operator representation, they share the important properties of their discrete counterparts: expressiveness, invariance under gauge transformations, simple rescaling flow, and compact expressions for the N-point functions of local observables. While we discuss mostly the continuous tensor network states extending projected entangled-pair states, we propose a generalization bearing similarities with the continuum multiscale entanglement renormalization ansatz.},
archivePrefix = {arXiv},
arxivId = {1808.00976},
author = {Tilloy, Antoine and Cirac, J. Ignacio},
doi = {10.1103/PhysRevX.9.021040},
eprint = {1808.00976},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2019 - Tilloy, Cirac - Continuous Tensor Network States for Quantum Fields.pdf:pdf},
issn = {21603308},
journal = {Phys. Rev. X},
keywords = {doi:10.1103/PhysRevX.9.021040 url:https://doi.org/,particles and fields,quantum physics,strongly correlated materials},
number = {2},
pages = {21040},
publisher = {American Physical Society},
title = {{Continuous Tensor Network States for Quantum Fields}},
url = {https://doi.org/10.1103/PhysRevX.9.021040},
volume = {9},
year = {2019}
}
@article{Jin2020a,
abstract = {Graph generation techniques are increasingly being adopted for drug discovery. Previous graph generation approaches have utilized relatively small molecular building blocks such as atoms or simple cycles, limiting their effectiveness to smaller molecules. Indeed, as we demonstrate, their performance degrades significantly for larger molecules. In this paper, we propose a new hierarchical graph encoder-decoder that employs significantly larger and more flexible graph motifs as basic building blocks. Our encoder produces a multi-resolution representation for each molecule in a fine-to-coarse fashion, from atoms to connected motifs. Each level integrates the encoding of constituents below with the graph at that level. Our autoregressive coarse-to-fine decoder adds one motif at a time, interleaving the decision of selecting a new motif with the process of resolving its attachments to the emerging molecule. We evaluate our model on multiple molecule generation tasks, including polymers, and show that our model significantly outperforms previous state-of-the-art baselines.},
archivePrefix = {arXiv},
arxivId = {2002.03230},
author = {Jin, Wengong and Barzilay, Regina and Jaakkola, Tommi},
eprint = {2002.03230},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2020 - Jin, Barzilay, Jaakkola - Hierarchical Generation of Molecular Graphs using Structural Motifs.pdf:pdf},
title = {{Hierarchical Generation of Molecular Graphs using Structural Motifs}},
url = {http://arxiv.org/abs/2002.03230},
year = {2020}
}
@article{Foreman-Mackey2013,
abstract = {We introduce a stable, well tested Python implementation of the affine-invariant ensemble sampler for Markov chain Monte Carlo (MCMC) proposed by Goodman {\&} Weare (2010). The code is open source and has already been used in several published projects in the astrophysics literature. The algorithm behind emcee has several advantages over traditional MCMC sampling methods and it has excellent performance as measured by the autocorrelation time (or function calls per independent sample). One major advantage of the algorithm is that it requires hand-tuning of only 1 or 2 parameters compared to {\$}\backslashsim N{\^{}}2{\$} for a traditional algorithm in an N-dimensional parameter space. In this document, we describe the algorithm and the details of our implementation and API. Exploiting the parallelism of the ensemble method, emcee permits any user to take advantage of multiple CPU cores without extra effort. The code is available online at http://dan.iel.fm/emcee under the MIT License.},
archivePrefix = {arXiv},
arxivId = {1202.3665},
author = {Foreman-Mackey, Daniel and Hogg, David W. and Lang, Dustin and Goodman, Jonathan},
doi = {10.1086/670067},
eprint = {1202.3665},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2013 - Foreman-Mackey et al. - emcee The MCMC Hammer.pdf:pdf},
issn = {00046280},
journal = {Publ. Astron. Soc. Pacific},
number = {925},
pages = {306--312},
title = {{emcee : The MCMC Hammer }},
volume = {125},
year = {2013}
}
@article{Vaswani2017,
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2017 - Vaswani et al. - Attention is all you need.pdf:pdf},
issn = {10495258},
journal = {Adv. Neural Inf. Process. Syst.},
number = {Nips},
pages = {5999--6009},
title = {{Attention is all you need}},
volume = {2017-Decem},
year = {2017}
}
@article{Frey2020,
abstract = {The discovery of intrinsic magnetic topological order in {\$}\backslashrm MnBi{\_}2Te{\_}4{\$} has invigorated the search for materials with coexisting magnetic and topological phases. These multi-order quantum materials are expected to exhibit new topological phases that can be tuned with magnetic fields, but the search for such materials is stymied by difficulties in predicting magnetic structure and stability. Here, we compute over 27,000 unique magnetic orderings for over 3,000 transition metal oxides in the Materials Project database to determine their magnetic ground states and estimate their effective exchange parameters and critical temperatures. We perform a high-throughput band topology analysis of centrosymmetric magnetic materials, calculate topological invariants, and identify 18 new candidate ferromagnetic topological semimetals, axion insulators, and antiferromagnetic topological insulators. To accelerate future efforts, machine learning classifiers are trained to predict both magnetic ground states and magnetic topological order without requiring first-principles calculations.},
archivePrefix = {arXiv},
arxivId = {2006.01075},
author = {Frey, Nathan C. and Horton, Matthew K. and Munro, Jason M. and Griffin, Sin{\'{e}}ad M. and Persson, Kristin A. and Shenoy, Vivek B.},
eprint = {2006.01075},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2020 - Frey et al. - High-throughput search for magnetic and topological order in transition metal oxides.pdf:pdf},
pages = {1--9},
title = {{High-throughput search for magnetic and topological order in transition metal oxides}},
url = {http://arxiv.org/abs/2006.01075},
year = {2020}
}
@article{Foley2019,
abstract = {Antiferromagnetism and d-wave superconductivity are the most important competing ground-state phases of cuprate superconductors. Using cellular dynamical mean-field theory for the Hubbard model, we revisit the question of the coexistence and competition of these phases in the one-band Hubbard model with realistic band parameters and interaction strengths. With an exact diagonalization solver, we improve on previous works with a more complete bath parametrization which is carefully chosen to grant the maximal possible freedom to the hybridization function for a given number of bath orbitals. Compared with previous incomplete parametrizations, this general bath parametrization shows that the range of microscopic coexistence of superconductivity and antiferromagnetism is reduced for band parameters for Nd2-xCexCuO4 (NCCO) and confined to electron doping with parameters relevant for YBa2Cu3O7-x (YBCO).},
archivePrefix = {arXiv},
arxivId = {1811.12363},
author = {Foley, A. and Verret, S. and Tremblay, A. M.S. and S{\'{e}}n{\'{e}}chal, D.},
doi = {10.1103/PhysRevB.99.184510},
eprint = {1811.12363},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2019 - Foley et al. - Coexistence of superconductivity and antiferromagnetism in the Hubbard model for cuprates.pdf:pdf},
issn = {24699969},
journal = {Phys. Rev. B},
number = {18},
pages = {1--11},
title = {{Coexistence of superconductivity and antiferromagnetism in the Hubbard model for cuprates}},
volume = {99},
year = {2019}
}
@article{Bradley2020,
abstract = {We present a framework for modeling words, phrases, and longer expressions in a natural language using reduced density operators. We show these operators capture something of the meaning of these expressions and, under the Loewner order on positive semidefinite operators, preserve both a simple form of entailment and the relevant statistics therein. Pulling back the curtain, the assignment is shown to be a functor between categories enriched over probabilities.},
archivePrefix = {arXiv},
arxivId = {2007.03834},
author = {Bradley, Tai-Danae and Vlassopoulos, Yiannis},
eprint = {2007.03834},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2020 - Bradley, Vlassopoulos - Language Modeling with Reduced Densities.pdf:pdf},
pages = {1--19},
title = {{Language Modeling with Reduced Densities}},
url = {http://arxiv.org/abs/2007.03834},
year = {2020}
}
@article{Yang2019a,
abstract = {Advancements in neural machinery have led to a wide range of algorithmic solutions for molecular property prediction. Two classes of models in particular have yielded promising results: neural networks applied to computed molecular fingerprints or expert-crafted descriptors and graph convolutional neural networks that construct a learned molecular representation by operating on the graph structure of the molecule. However, recent literature has yet to clearly determine which of these two methods is superior when generalizing to new chemical space. Furthermore, prior research has rarely examined these new models in industry research settings in comparison to existing employed models. In this paper, we benchmark models extensively on 19 public and 16 proprietary industrial data sets spanning a wide variety of chemical end points. In addition, we introduce a graph convolutional model that consistently matches or outperforms models using fixed molecular descriptors as well as previous graph neural architectures on both public and proprietary data sets. Our empirical findings indicate that while approaches based on these representations have yet to reach the level of experimental reproducibility, our proposed model nevertheless offers significant improvements over models currently used in industrial workflows.},
annote = {Chemprop},
archivePrefix = {arXiv},
arxivId = {1904.01561},
author = {Yang, Kevin and Swanson, Kyle and Jin, Wengong and Coley, Connor and Eiden, Philipp and Gao, Hua and Guzman-Perez, Angel and Hopper, Timothy and Kelley, Brian and Mathea, Miriam and Palmer, Andrew and Settels, Volker and Jaakkola, Tommi and Jensen, Klavs and Barzilay, Regina},
doi = {10.1021/acs.jcim.9b00237},
eprint = {1904.01561},
file = {:Users/Simon/Documents/papiers{\_}mendeley//2019 - Yang et al. - Analyzing Learned Molecular Representations for Property Prediction.pdf:pdf},
issn = {15205142},
journal = {J. Chem. Inf. Model.},
month = {aug},
number = {8},
pages = {3370--3388},
publisher = {American Chemical Society},
title = {{Analyzing Learned Molecular Representations for Property Prediction}},
volume = {59},
year = {2019}
}
@article{Belkin2019a,
abstract = {Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias-variance trade-off, appears to be at odds with the observed behavior of methods used in the modern machine learning practice. The bias-variance trade-off implies that a model should balance under-fitting and over-fitting: rich enough to express underlying structure in data, simple enough to avoid fitting spurious patterns. However, in the modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered over-fit, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This "double descent" curve subsumes the textbook U-shaped bias-variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine learning models delineates the limits of classical analyses, and has implications for both the theory and practice of machine learning.},
archivePrefix = {arXiv},
arxivId = {1812.11118},
author = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
doi = {10.1073/pnas.1903070116},
eprint = {1812.11118},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2018 - Belkin et al. - Reconciling modern machine learning practice and the bias-variance trade-off.pdf:pdf},
issn = {0027-8424},
journal = {Proc. Natl. Acad. Sci.},
keywords = {Bias–variance trade-off,Machine learning,Neural networks},
month = {dec},
number = {32},
pages = {15849--15854},
title = {{Reconciling modern machine learning practice and the bias-variance trade-off}},
url = {http://arxiv.org/abs/1812.11118 http://www.pnas.org/lookup/doi/10.1073/pnas.1903070116},
volume = {116},
year = {2018}
}
@article{Alon2020,
abstract = {Graph neural networks (GNNs) were shown to effectively learn from highly structured data containing elements (nodes) with relationships (edges) between them. GNN variants differ in how each node in the graph absorbs the information flowing from its neighbor nodes. In this paper, we highlight an inherent problem in GNNs: the mechanism of propagating information between neighbors creates a bottleneck when every node aggregates messages from its neighbors. This bottleneck causes the over-squashing of exponentially-growing information into fixed-size vectors. As a result, the graph fails to propagate messages flowing from distant nodes and performs poorly when the prediction task depends on long-range information. We demonstrate that the bottleneck hinders popular GNNs from fitting the training data. We show that GNNs that absorb incoming edges equally, like GCN and GIN, are more susceptible to over-squashing than other GNN types. We further show that existing, extensively-tuned, GNN-based models suffer from over-squashing and that breaking the bottleneck improves state-of-the-art results without any hyperparameter tuning or additional weights.},
archivePrefix = {arXiv},
arxivId = {2006.05205},
author = {Alon, Uri and Yahav, Eran},
eprint = {2006.05205},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2020 - Alon, Yahav - On the Bottleneck of Graph Neural Networks and its Practical Implications.pdf:pdf},
title = {{On the Bottleneck of Graph Neural Networks and its Practical Implications}},
url = {http://arxiv.org/abs/2006.05205},
year = {2020}
}
@misc{Flohr2004,
abstract = {The aim of scaffold hopping is to discover structurally novel compounds starting from known active compounds by modifying the central core structure of the molecule. Scaffold hopping is a central task of modern medicinal chemistry requiring a multitude of techniques, which are discussed in this article. Their application has led to several molecules with chemically completely different core structures, and yet binding to the same receptor. Computational approaches for scaffold hopping highlight the challenges of the field that are still unsolved. {\textcopyright} 2004 Elsevier Ltd. All rights reserved.},
author = {B{\"{o}}hm, Hans Joachim and Flohr, Alexander and Stahl, Martin},
booktitle = {Drug Discov. Today Technol.},
doi = {10.1016/j.ddtec.2004.10.009},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2004 - B{\"{o}}hm, Flohr, Stahl - Scaffold hopping.pdf:pdf},
issn = {17406749},
number = {3},
pages = {217--224},
title = {{Scaffold hopping}},
volume = {1},
year = {2004}
}
@phdthesis{Verret2018,
abstract = {Depuis leur d{\'{e}}couverte, les ondes de densit{\'{e}} occupent une place de plus en plus importante dans notre compr{\'{e}}hension de la supraconductivit{\'{e}} {\`{a}} haute temp{\'{e}}rature critique. Leur r{\^{o}}le demeure toutefois incertain pour plusieurs observations exp{\'{e}}rimentales. L'objectif de cette th{\`{e}}se est de d{\'{e}}terminer si les ondes de densit{\'{e}} peuvent expliquer certaines de ces observations. Ces derni{\`{e}}res incluent (i) le gap partiel (pseudogap) mesur{\'{e}} dans la densit{\'{e}} d'{\'{e}}tats, (ii) les sous-structures du gap supraconducteur observ{\'{e}}es dans les coeurs de vortex, et (iii) l'augmentation rapide de p {\`{a}} 1 + p dans les mesures du nombre de Hall au dopage p*. Huit mod{\`{e}}les ph{\'{e}}nom{\'{e}}nologiques de type champ-moyen sont pr{\'{e}}sent{\'{e}}s pour {\'{e}}tudier ces ph{\'{e}}nom{\`{e}}nes. Les r{\'{e}}sultats indiquent que l'approximation du champ moyen pour les ondes de densit{\'{e}} observ{\'{e}}es dans les cuprates ne peut pas expliquer le gap partiel dans la densit{\'{e}} d'{\'{e}}tats, mais elle fournit une bonne explication des sous-structures du gap supraconducteur. Pour reproduire l'augmentation rapide du nombre de Hall, la d{\'{e}}marche propos{\'{e}}e par Storey est suivie avec les mod{\`{e}}les ph{\'{e}}nom{\'{e}}nologiques d'antiferromagn{\'{e}}tisme, d'antiferromagn{\'{e}}tisme spiral et la th{\'{e}}orie Yang-Rice-Zhang. En comparant ces trois mod{\`{e}}les {\`{a}} un mod{\`{e}}le d'onde de densit{\'{e}} de spin incommensurable colin{\'{e}}aire, on identifie le point commun qui fait leur force: ils s{\'{e}}parent la dispersion du mod{\`{e}}le {\`{a}} une bande des cuprates en deux nouvelles bandes. Pour mettre ces mod{\`{e}}les {\`{a}} l'{\'{e}}preuve, leur pr{\'{e}}dictions pour la chaleur sp{\'{e}}cifique et l'effet Seebeck sont obtenues afin de les comparer une {\'{e}}ventuelle mesure exp{\'{e}}rimentale dans les m{\^{e}}me conditions que pour le nombre de Hall.},
author = {Verret, Simon},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2018 - Verret - R{\^{o}}le des ondes de densit{\'{e}} dans les mod{\`{e}}les th{\'{e}}oriques pour cuprates supraconducteurs.pdf:pdf},
pages = {119},
school = {Universit{\'{e}} de Sherbrooke},
title = {{R{\^{o}}le des ondes de densit{\'{e}} dans les mod{\`{e}}les th{\'{e}}oriques pour cuprates supraconducteurs}},
type = {PhD},
url = {https://savoirs.usherbrooke.ca/handle/11143/12097},
year = {2018}
}
@article{Doha2011,
abstract = {A new explicit formula for the integrals of Bernstein polynomials of any degree for any order in terms of Bernstein polynomials themselves is derived. A fast and accurate algorithm is developed for the solution of high even-order boundary value problems (BVPs) with two point boundary conditions but by considering their integrated forms. The BernsteinPetrovGalerkin method (BPG) is applied to construct the numerical solution for such problems. The method is then tested on examples and compared with other methods. It is shown that the BPG yields better results. {\textcopyright} 2010 Elsevier Ltd. All rights reserved.},
author = {Doha, E. H. and Bhrawy, A. H. and Saker, M. A.},
doi = {10.1016/j.aml.2010.11.013},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2011 - Doha, Bhrawy, Saker - Integrals of Bernstein polynomials An application for the solution of high even-order differential equation.pdf:pdf},
issn = {08939659},
journal = {Appl. Math. Lett.},
keywords = {Bernstein polynomials,High order differential equations,Spectral method},
number = {4},
pages = {559--565},
publisher = {Elsevier Ltd},
title = {{Integrals of Bernstein polynomials: An application for the solution of high even-order differential equations}},
url = {http://dx.doi.org/10.1016/j.aml.2010.11.013},
volume = {24},
year = {2011}
}
@article{Lyu2019a,
abstract = {Despite intense interest in expanding chemical space, libraries containing hundreds-of-millions to billions of diverse molecules have remained inaccessible. Here we investigate structure-based docking of 170 million make-on-demand compounds from 130 well-characterized reactions. The resulting library is diverse, representing over 10.7 million scaffolds that are otherwise unavailable. For each compound in the library, docking against AmpC $\beta$-lactamase (AmpC) and the D 4 dopamine receptor were simulated. From the top-ranking molecules, 44 and 549 compounds were synthesized and tested for interactions with AmpC and the D 4 dopamine receptor, respectively. We found a phenolate inhibitor of AmpC, which revealed a group of inhibitors without known precedent. This molecule was optimized to 77 nM, which places it among the most potent non-covalent AmpC inhibitors known. Crystal structures of this and other AmpC inhibitors confirmed the docking predictions. Against the D 4 dopamine receptor, hit rates fell almost monotonically with docking score, and a hit-rate versus score curve predicted that the library contained 453,000 ligands for the D 4 dopamine receptor. Of 81 new chemotypes discovered, 30 showed submicromolar activity, including a 180-pM subtype-selective agonist of the D 4 dopamine receptor.},
annote = {Description of Docking screening on D4 and AmpC receptors from the ZINC library},
author = {Lyu, Jiankun and Wang, Sheng and Balius, Trent E. and Singh, Isha and Levit, Anat and Moroz, Yurii S. and O'Meara, Matthew J. and Che, Tao and Algaa, Enkhjargal and Tolmachova, Kateryna and Tolmachev, Andrey A. and Shoichet, Brian K. and Roth, Bryan L. and Irwin, John J.},
doi = {10.1038/s41586-019-0917-9},
file = {:Users/Simon/Documents/papiers{\_}mendeley//2019 - Lyu et al. - Ultra-large library docking for discovering new chemotypes.pdf:pdf},
issn = {14764687},
journal = {Nature},
number = {7743},
pages = {224--229},
publisher = {Springer US},
title = {{Ultra-large library docking for discovering new chemotypes}},
url = {http://dx.doi.org/10.1038/s41586-019-0917-9},
volume = {566},
year = {2019}
}
@article{Charlebois2017,
abstract = {The presence of incommensurate spiral spin-density waves (SDW) has been proposed to explain the p (hole doping) to 1+p jump measured in the Hall number nH at a doping p∗. Here we explore incommensurate collinear SDW as another possible explanation of this phenomenon, distinct from the incommensurate spiral SDW proposal. We examine the effect of different SDW strengths and wave vectors, and we find that the nH∼p behavior is hardly reproduced at low doping. Furthermore, the calculated nH and Fermi surfaces give characteristic features that should be observed; thus, the lack of these features in experiment suggests that the incommensurate collinear SDW is unlikely to be a good candidate to explain the nH∼p observed in the pseudogap regime.},
author = {Charlebois, M. and Verret, S. and Foley, A. and Simard, O. and S{\'{e}}n{\'{e}}chal, D. and Tremblay, A. M.S.},
doi = {10.1103/PhysRevB.96.205132},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2017 - Charlebois et al. - Hall effect in cuprates with an incommensurate collinear spin-density wave.pdf:pdf},
issn = {24699969},
journal = {Phys. Rev. B},
number = {20},
pages = {1--9},
title = {{Hall effect in cuprates with an incommensurate collinear spin-density wave}},
volume = {96},
year = {2017}
}
@article{Xie2019,
abstract = {We propose a data-driven learning framework for the analytic continuation problem in numerical quantum many-body physics. Designing an accurate and efficient framework for the analytic continuation of imaginary time using computational data is a grand challenge that has hindered meaningful links with experimental data. The standard Maximum Entropy (MaxEnt)-based method is limited by the quality of the computational data and the availability of prior information. Also, the MaxEnt is not able to solve the inversion problem under high level of noise in the data. Here we introduce a novel learning model for the analytic continuation problem using a Adams-Bashforth residual neural network (AB-ResNet). The advantage of this deep learning network is that it is model independent and, therefore, does not require prior information concerning the quantity of interest given by the spectral function. More importantly, the ResNet-based model achieves higher accuracy than MaxEnt for data with higher level of noise. Finally, numerical examples show that the developed AB-ResNet is able to recover the spectral function with accuracy comparable to MaxEnt where the noise level is relatively small.},
archivePrefix = {arXiv},
arxivId = {1905.10430},
author = {Xie, Xuping and Bao, Feng and Maier, Thomas and Webster, Clayton},
eprint = {1905.10430},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2019 - Xie et al. - Analytic Continuation of Noisy Data Using Adams Bashforth ResNet.pdf:pdf},
number = {1},
pages = {1--8},
title = {{Analytic Continuation of Noisy Data Using Adams Bashforth ResNet}},
url = {http://arxiv.org/abs/1905.10430},
year = {2019}
}
@article{Sun2019,
abstract = {This paper studies learning the representations of whole graphs in both unsupervised and semi-supervised scenarios. Graph-level representations are critical in a variety of real-world applications such as predicting the properties of molecules and community analysis in social networks. Traditional graph kernel based methods are simple, yet effective for obtaining fixed-length representations for graphs but they suffer from poor generalization due to hand-crafted designs. There are also some recent methods based on language models (e.g. graph2vec) but they tend to only consider certain substructures (e.g. subtrees) as graph representatives. Inspired by recent progress of unsupervised representation learning, in this paper we proposed a novel method called InfoGraph for learning graph-level representations. We maximize the mutual information between the graph-level representation and the representations of substructures of different scales (e.g., nodes, edges, triangles). By doing so, the graph-level representations encode aspects of the data that are shared across different scales of substructures. Furthermore, we further propose InfoGraph*, an extension of InfoGraph for semi-supervised scenarios. InfoGraph* maximizes the mutual information between unsupervised graph representations learned by InfoGraph and the representations learned by existing supervised methods. As a result, the supervised encoder learns from unlabeled data while preserving the latent semantic space favored by the current supervised task. Experimental results on the tasks of graph classification and molecular property prediction show that InfoGraph is superior to state-of-the-art baselines and InfoGraph* can achieve performance competitive with state-of-the-art semi-supervised models.},
archivePrefix = {arXiv},
arxivId = {1908.01000},
author = {Sun, Fan-Yun and Hoffmann, Jordan and Verma, Vikas and Tang, Jian},
eprint = {1908.01000},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2019 - Sun et al. - InfoGraph Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization.pdf:pdf},
number = {2018},
pages = {1--16},
title = {{InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization}},
url = {http://arxiv.org/abs/1908.01000},
year = {2019}
}
@article{Jin2018c,
abstract = {We view molecular optimization as a graph-to-graph translation problem. The goal is to learn to map from one molecular graph to another with better properties based on an available corpus of paired molecules. Since molecules can be optimized in different ways, there are multiple viable translations for each input graph. A key challenge is therefore to model diverse translation outputs. Our primary contributions include a junction tree encoder-decoder for learning diverse graph translations along with a novel adversarial training method for aligning distributions of molecules. Diverse output distributions in our model are explicitly realized by low-dimensional latent vectors that modulate the translation process. We evaluate our model on multiple molecular optimization tasks and show that our model outperforms previous state-of-the-art baselines.},
annote = {From Duplicate 1 (Learning Multimodal Graph-to-Graph Translation for Molecular Optimization - Jin, Wengong; Yang, Kevin; Barzilay, Regina; Jaakkola, Tommi)

From Duplicate 2 (Learning Multimodal Graph-to-Graph Translation for Molecular Optimization - Jin, Wengong; Yang, Kevin; Barzilay, Regina; Jaakkola, Tommi)

Junction Tree translation to better molecule. Should recheck the related work section.
Didn't read the model itself},
archivePrefix = {arXiv},
arxivId = {1812.01070},
author = {Jin, Wengong and Yang, Kevin and Barzilay, Regina and Jaakkola, Tommi},
eprint = {1812.01070},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2018 - Jin et al. - Learning Multimodal Graph-to-Graph Translation for Molecular Optimization.pdf:pdf},
journal = {7th Int. Conf. Learn. Represent. ICLR 2019},
month = {dec},
publisher = {International Conference on Learning Representations, ICLR},
title = {{Learning Multimodal Graph-to-Graph Translation for Molecular Optimization}},
url = {http://arxiv.org/abs/1812.01070},
year = {2018}
}
@article{Georges1992,
abstract = {We present an exact mapping of the Hubbard model in infinite dimensions onto a single-impurity Anderson (or Wolff) model supplemented by a self-consistency condition. This provides a mean-field picture of strongly corrrelated systems, which becomes exact as d. We point out a special integrable case of the mean-field equations, and study the general case using a perturbative renormalization group around the atomic limit. Three distinct Fermi-liquid regimes arise, corresponding to the Kondo, mixed-valence, and empty-orbitals regimes of the single-impurity problem. The Kondo resonance and the satellite peaks of the single-impurity model correspond to the quasiparticle and Hubbard-bands features of the Hubbard model, respectively. {\textcopyright} 1992 The American Physical Society.},
author = {Georges, Antoine and Kotliar, Gabriel},
doi = {10.1103/PhysRevB.45.6479},
file = {:Users/Simon/Documents/papiers{\_}mendeley/1992 - Georges, Kotliar - Hubbard model in infinite dimensions.pdf:pdf},
issn = {01631829},
journal = {Phys. Rev. B},
number = {12},
pages = {6479--6483},
title = {{Hubbard model in infinite dimensions}},
volume = {45},
year = {1992}
}
@article{Arrivault2016,
abstract = {Sp2Learn is a Python toolbox for the spectral learning of weighted automata from a set of strings, licensed under Free BSD. This paper gives the main formal ideas behind the spectral learning algorithm and details the content of the toolbox. Use cases and an experimental section are also provided.},
author = {Arrivault, Denis and Benielli, Dominique and Denis, Fran{\c{c}}ois and Eyraud, R{\'{e}}mi},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2016 - Arrivault et al. - Sp2Learn A Toolbox for the spectral learning of weighted automata.pdf:pdf},
journal = {JMLR Work. Conf. Proc.},
keywords = {Spectral Learning,Toolbox,Weighted Automata},
pages = {105--119},
title = {{Sp2Learn: A Toolbox for the spectral learning of weighted automata *}},
url = {https://github.com/ICML14MoMCompare/spectral-learn.},
volume = {57},
year = {2016}
}
@article{Verret2019,
abstract = {It is well known that cellular dynamical mean-field theory (CDMFT) leads to the artificial breaking of translation invariance. In spite of this, it is one of the most successful methods to treat strongly correlated electrons systems. Here, we investigate in more detail how this broken translation invariance manifests itself. This allows us to disentangle artificial broken translation invariance effects from the genuine strongly correlated effects captured by CDMFT. We report artificial density waves taking the shape of the cluster-cluster density waves-in all our zero temperature CDMFT solutions, including pair density waves in the superconducting state. We discuss the limitations of periodization regarding this phenomenon, and we present mean-field density-wave models that reproduce CDMFT results at low energy in the superconducting state. We then discuss how these artificial density waves help the agreement of CDMFT with high temperature superconducting cuprates regarding the low-energy spectrum, in particular for subgap structures observed in tunneling microscopy. We relate these subgap structures to nodal and antinodal gaps in our results, similar to those observed in photoemission experiments. This fortuitous agreement suggests that spatial inhomogeneity may be a key ingredient to explain some features of the low-energy underdoped spectrum of cuprates with strongly correlated methods. This work deepens our understanding of CDMFT and clearly identifies signatures of broken translation invariance in the presence of strong correlations.},
author = {Verret, S. and Roy, J. and Foley, A. and Charlebois, M. and S{\'{e}}n{\'{e}}chal, D. and Tremblay, A. M.S.},
doi = {10.1103/PhysRevB.100.224520},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2019 - Verret et al. - Intrinsic cluster-shaped density waves in cellular dynamical mean-field theory.pdf:pdf},
issn = {24699969},
journal = {Phys. Rev. B},
keywords = {doi:10.1103/PhysRevB.100.224520 url:https://doi.or},
number = {22},
pages = {1--14},
publisher = {American Physical Society},
title = {{Intrinsic cluster-shaped density waves in cellular dynamical mean-field theory}},
volume = {100},
year = {2019}
}
@article{Kontani2020a,
abstract = {We predict the theoretical occurrence of nanoscale spontaneous spin-current, called the spin loop-current (sLC) order, as a promising origin of the pseudogap and electronic nematicity in cuprates. We demonstrate that the spontaneous sLC is accompanied by the exotic odd-parity electron-hole pairs that are mediated by transverse spin fluctuations around the pseudogap temperature {\$}T{\^{}}*{\$}. The present theory predicts the occurrence of the condensation of odd-parity magnon pairs simultaneously. The sLC order is ``hidden'' in that neither internal magnetic field nor charge density modulation is induced, whereas the predicted sLC with finite wavenumber naturally gives the Fermi arc structure. In addition, the fluctuations of sLC order work as attractive pairing interaction between adjacent hot spots, which enlarges the {\$}d{\$}-wave superconducting transition temperature {\$}T{\_}c{\$}. Thus, the sLC state will be the key ingredient in understanding pseudogap, electronic nematicity as well as superconductivity in cuprates and other strongly correlated metals.},
archivePrefix = {arXiv},
arxivId = {2003.07556},
author = {Kontani, Hiroshi and Yamakawa, Youichi and Tazai, Rina and Onari, Seiichiro},
eprint = {2003.07556},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2020 - Kontani et al. - Spontaneous spin-loop-current order mediated by transverse spin fluctuations in cuprate superconductors.pdf:pdf},
title = {{Spontaneous spin-loop-current order mediated by transverse spin fluctuations in cuprate superconductors}},
url = {http://arxiv.org/abs/2003.07556},
year = {2020}
}
@article{Fournier2020,
abstract = {Inverse problems are encountered in many domains of physics, with analytic continuation of the imaginary Green's function into the real frequency domain being a particularly important example. However, the analytic continuation problem is ill defined and currently no analytic transformation for solving it is known. We present a general framework for building an artificial neural network (ANN) that solves this task with a supervised learning approach. Application of the ANN approach to quantum Monte Carlo calculations and simulated Green's function data demonstrates its high accuracy. By comparing with the commonly used maximum entropy approach, we show that our method can reach the same level of accuracy for low-noise input data, while performing significantly better when the noise strength increases. The computational cost of the proposed neural network approach is reduced by almost three orders of magnitude compared to the maximum entropy method.},
archivePrefix = {arXiv},
arxivId = {1810.00913},
author = {Fournier, Romain and Wang, Lei and Yazyev, Oleg V. and Wu, Quan Sheng},
doi = {10.1103/PhysRevLett.124.056401},
eprint = {1810.00913},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2020 - Fournier et al. - Artificial Neural Network Approach to the Analytic Continuation Problem(2).pdf:pdf;:Users/Simon/Documents/papiers{\_}mendeley/2020 - Fournier et al. - Artificial Neural Network Approach to the Analytic Continuation Problem(3).pdf:pdf;:Users/Simon/Documents/papiers{\_}mendeley/2020 - Fournier et al. - Artificial Neural Network Approach to the Analytic Continuation Problem.pdf:pdf},
issn = {10797114},
journal = {Phys. Rev. Lett.},
number = {5},
pages = {1--5},
pmid = {32083907},
title = {{Artificial Neural Network Approach to the Analytic Continuation Problem}},
volume = {124},
year = {2020}
}
@article{Rumetshofer2019,
abstract = {Bayesian parametric analytic continuation (BPAC) is proposed for the analytic continuation of noisy imaginary-time Green's function data as, e.g., obtained by continuous-time quantum Monte Carlo simulations (CTQMC). Within BPAC, the spectral function is inferred from a suitable set of parametrized basis functions. Bayesian model comparison then allows to assess the reliability of different parametrizations. The required evidence integrals of such a model comparison are determined by nested sampling. Compared to the maximum entropy method (MEM), routinely used for the analytic continuation of CTQMC data, the presented approach allows to infer whether the data support specific structures of the spectral function. We demonstrate the capability of BPAC in terms of CTQMC data for an Anderson impurity model (AIM) that shows a generalized Kondo scenario and compare the BPAC reconstruction to the MEM as well as to the spectral function obtained from the real-time fork tensor product state impurity solver where no analytic continuation is required. Furthermore, we present a combination of MEM and BPAC and its application to an AIM arising from the ab initio treatment of SrVO3.},
archivePrefix = {arXiv},
arxivId = {1906.03396},
author = {Rumetshofer, Michael and Bauernfeind, Daniel and {Von Der Linden}, Wolfgang},
doi = {10.1103/PhysRevB.100.075137},
eprint = {1906.03396},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2019 - Rumetshofer, Bauernfeind, Von Der Linden - Bayesian parametric analytic continuation of Green's functions.pdf:pdf},
issn = {24699969},
journal = {Phys. Rev. B},
keywords = {doi:10.1103/PhysRevB.100.075137 url:https://doi.or},
number = {7},
pages = {1--11},
publisher = {American Physical Society},
title = {{Bayesian parametric analytic continuation of Green's functions}},
volume = {100},
year = {2019}
}
@article{PhysRevB.45.6479,
author = {Georges, Antoine and Kotliar, Gabriel},
doi = {10.1103/PhysRevB.45.6479},
file = {:Users/Simon/Documents/papiers{\_}mendeley/1992 - Georges, Kotliar - Hubbard model in infinite dimensions.pdf:pdf},
issn = {0163-1829},
journal = {Phys. Rev. B},
month = {mar},
number = {12},
pages = {6479--6483},
publisher = {American Physical Society},
title = {{Hubbard model in infinite dimensions}},
url = {https://link.aps.org/doi/10.1103/PhysRevB.45.6479},
volume = {45},
year = {1992}
}
@article{Levine2019,
abstract = {Modern deep learning has enabled unprecedented achievements in various domains. Nonetheless, employment of machine learning for wave function representations is focused on more traditional architectures such as restricted Boltzmann machines (RBMs) and fully connected neural networks. In this Letter, we establish that contemporary deep learning architectures, in the form of deep convolutional and recurrent networks, can efficiently represent highly entangled quantum systems. By constructing tensor network equivalents of these architectures, we identify an inherent reuse of information in the network operation as a key trait which distinguishes them from standard tensor network-based representations, and which enhances their entanglement capacity. Our results show that such architectures can support volume-law entanglement scaling, polynomially more efficiently than presently employed RBMs. Thus, beyond a quantification of the entanglement capacity of leading deep learning architectures, our analysis formally motivates a shift of trending neural-network-based wave function representations closer to the state-of-the-art in machine learning.},
archivePrefix = {arXiv},
arxivId = {1803.09780},
author = {Levine, Yoav and Sharir, Or and Cohen, Nadav and Shashua, Amnon},
doi = {10.1103/PhysRevLett.122.065301},
eprint = {1803.09780},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2019 - Levine et al. - Quantum Entanglement in Deep Learning Architectures.pdf:pdf},
issn = {10797114},
journal = {Phys. Rev. Lett.},
number = {6},
pages = {1--9},
pmid = {30822082},
title = {{Quantum Entanglement in Deep Learning Architectures}},
volume = {122},
year = {2019}
}
@article{Ghanem2020,
abstract = {The average spectrum method is a promising approach for the analytic continuation of imaginary time or frequency data to the real axis. It determines the analytic continuation of noisy data from a functional average over all admissible spectral functions, weighted by how well they fit the data. Its main advantage is the apparent lack of adjustable parameters and smoothness constraints, using instead the information on the statistical noise in the data. Its main disadvantage is the enormous computational cost of performing the functional integral. Here we introduce an efficient implementation, based on the singular value decomposition of the integral kernel, eliminating this problem. It allows us to analyze the behavior of the average spectrum method in detail. We find that the discretization of the real-frequency grid, on which the spectral function is represented, biases the results. The distribution of the grid points plays the role of a default model while the number of grid points acts as a regularization parameter. We give a quantitative explanation for this behavior, point out the crucial role of the default model and provide a practical method for choosing it, making the average spectrum method a reliable and efficient technique for analytic continuation.},
author = {Ghanem, Khaldoon and Koch, Erik},
doi = {10.1103/PhysRevB.101.085111},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2020 - Ghanem, Koch - Average spectrum method for analytic continuation Efficient blocked-mode sampling and dependence on the discretiza.pdf:pdf},
issn = {24699969},
journal = {Phys. Rev. B},
keywords = {doi:10.1103/PhysRevB.101.085111 url:https://doi.or},
number = {8},
pages = {1--11},
publisher = {American Physical Society},
title = {{Average spectrum method for analytic continuation: Efficient blocked-mode sampling and dependence on the discretization grid}},
volume = {101},
year = {2020}
}
@article{Fang2020,
abstract = {The nature of the pseudogap phase remains a major barrier to our understanding of cuprate high-temperature superconductivity. Whether or not this metallic phase is defined by any of the reported broken symmetries, the topology of its Fermi surface remains a fundamental open question. Here we use angle-dependent magnetoresistance (ADMR) to measure the Fermi surface of the cuprate Nd-LSCO. Above the critical doping p*-outside of the pseudogap phase-we find a Fermi surface that is in quantitative agreement with angle-resolved photoemission. Below p*, however, the ADMR is qualitatively different, revealing a clear change in Fermi surface topology. We find that our data is most consistent with a Fermi surface that has been reconstructed by a {\$}Q=(\backslashpi, \backslashpi){\$} wavevector. While static {\$}Q=(\backslashpi, \backslashpi){\$} antiferromagnetism is not found at these dopings, our results suggest that this wavevector is a fundamental organizing principle of the pseudogap phase.},
archivePrefix = {arXiv},
arxivId = {2004.01725},
author = {Fang, Yawen and Grissonnanche, Gael and Legros, Anaelle and Verret, Simon and Laliberte, Francis and Collignon, Clement and Ataei, Amirreza and Dion, Maxime and Zhou, Jianshi and Graf, David and Lawler, M. J. and Goddard, Paul and Taillefer, Louis and Ramshaw, B. J.},
eprint = {2004.01725},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2020 - Fang et al. - Fermi surface transformation at the pseudogap critical point of a cuprate superconductor.pdf:pdf},
pages = {1--37},
title = {{Fermi surface transformation at the pseudogap critical point of a cuprate superconductor}},
url = {http://arxiv.org/abs/2004.01725},
year = {2020}
}
@article{Yoon2018,
abstract = {We present a machine-learning approach to a long-standing issue in quantum many-body physics, namely, analytic continuation. This notorious ill-conditioned problem of obtaining spectral function from an imaginary time Green's function has been a focus of new method developments for past decades. Here we demonstrate the usefulness of modern machine-learning techniques including convolutional neural networks and the variants of a stochastic gradient descent optimizer. The machine-learning continuation kernel is successfully realized without any "domain knowledge," which means that any physical "prior" is not utilized in the kernel construction and the neural networks "learn" the knowledge solely from "training." The outstanding performance is achieved for both insulating and metallic band structure. Our machine-learning-based approach not only provides the more accurate spectrum than the conventional methods in terms of peak positions and heights, but is also more robust against the noise which is the required key feature for any continuation technique to be successful. Furthermore, its computation speed is 104-105 times faster than the maximum entropy method.},
archivePrefix = {arXiv},
arxivId = {1806.03841},
author = {Yoon, Hongkee and Sim, Jae Hoon and Han, Myung Joon},
doi = {10.1103/PhysRevB.98.245101},
eprint = {1806.03841},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2018 - Yoon, Sim, Han - Analytic continuation via domain knowledge free machine learning.pdf:pdf},
issn = {24699969},
journal = {Phys. Rev. B},
keywords = {doi:10.1103/PhysRevB.98.245101 url:https://doi.org},
number = {24},
pages = {1--7},
publisher = {American Physical Society},
title = {{Analytic continuation via domain knowledge free machine learning}},
volume = {98},
year = {2018}
}
@article{Saito2019,
abstract = {A mechanism for the phonon Hall effect (PHE) in nonmagnetic insulators under an external magnetic field is theoretically studied. PHE is known in (para)magnetic compounds, where the magnetic moments and spin-orbit interaction play an essential role. In sharp contrast, we here discuss that the PHE also occurs in nonmagnetic band insulators subject to the magnetic field. We find that a correction to the Born-Oppenheimer approximation gives rise to a Raman-type interaction between the magnetic field and the phonons; this interaction gives rise to the Berry curvature of a phonon band. This Berry curvature results in the finite thermal Hall conductivity $\kappa$H in nonmagnetic band insulators. The value of $\kappa$H is calculated for square and honeycomb lattices. The order of the magnitude estimation for $\kappa$H is given for Si at room temperature.},
archivePrefix = {arXiv},
arxivId = {1901.08850},
author = {Saito, Takuma and Misaki, Kou and Ishizuka, Hiroaki and Nagaosa, Naoto},
doi = {10.1103/PhysRevLett.123.255901},
eprint = {1901.08850},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2019 - Saito et al. - Berry Phase of Phonons and Thermal Hall Effect in Nonmagnetic Insulators.pdf:pdf},
issn = {10797114},
journal = {Phys. Rev. Lett.},
keywords = {doi:10.1103/PhysRevLett.123.255901 url:https://doi},
number = {25},
pages = {255901},
publisher = {American Physical Society},
title = {{Berry Phase of Phonons and Thermal Hall Effect in Nonmagnetic Insulators}},
url = {https://doi.org/10.1103/PhysRevLett.123.255901},
volume = {123},
year = {2019}
}
@article{Jang2019a,
abstract = {Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.},
archivePrefix = {arXiv},
arxivId = {1611.01144},
author = {Jang, Eric and Gu, Shixiang and Poole, Ben},
eprint = {1611.01144},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2019 - Jang, Gu, Poole - Categorical reparameterization with gumbel-softmax.pdf:pdf},
journal = {5th Int. Conf. Learn. Represent. ICLR 2017 - Conf. Track Proc.},
pages = {1--13},
title = {{Categorical reparameterization with gumbel-softmax}},
year = {2019}
}
@article{Michon2019,
abstract = {The three central phenomena of cuprate (copper oxide) superconductors are linked by a common doping level p*—at which the enigmatic pseudogap phase ends and the resistivity exhibits an anomalous linear dependence on temperature, and around which the superconducting phase forms a dome-shaped area in the phase diagram1. However, the fundamental nature of p* remains unclear, in particular regarding whether it marks a true quantum phase transition. Here we measure the specific heat C of the cuprates Eu-LSCO and Nd-LSCO at low temperature in magnetic fields large enough to suppress superconductivity, over a wide doping range2 that includes p*. As a function of doping, we find that Cel/T is strongly peaked at p* (where Cel is the electronic contribution to C) and exhibits a log(1/T) dependence as temperature T tends to zero. These are the classic thermodynamic signatures of a quantum critical point3–5, as observed in heavy-fermion6 and iron-based7 superconductors at the point where their antiferromagnetic phase comes to an end. We conclude that the pseudogap phase of cuprates ends at a quantum critical point, the associated fluctuations of which are probably involved in d-wave pairing and the anomalous scattering of charge carriers.},
author = {Michon, B. and Girod, C. and Badoux, S. and Ka{\v{c}}mar{\v{c}}{\'{i}}k, J. and Ma, Q. and Dragomir, M. and Dabkowska, H. A. and Gaulin, B. D. and Zhou, J. S. and Pyon, S. and Takayama, T. and Takagi, H. and Verret, S. and Doiron-Leyraud, N. and Marcenat, C. and Taillefer, L. and Klein, T.},
doi = {10.1038/s41586-019-0932-x},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2019 - Michon et al. - Thermodynamic signatures of quantum criticality in cuprate superconductors.pdf:pdf},
issn = {14764687},
journal = {Nature},
number = {7747},
pages = {218--222},
pmid = {30760922},
title = {{Thermodynamic signatures of quantum criticality in cuprate superconductors}},
volume = {567},
year = {2019}
}
@article{Zhou2019b,
abstract = {We present a framework, which we call Molecule Deep Q-Networks (MolDQN), for molecule optimization by combining domain knowledge of chemistry and state-of-the-art reinforcement learning techniques (double Q-learning and randomized value functions). We directly define modifications on molecules, thereby ensuring 100{\%} chemical validity. Further, we operate without pre-training on any dataset to avoid possible bias from the choice of that set. MolDQN achieves comparable or better performance against several other recently published algorithms for benchmark molecular optimization tasks. However, we also argue that many of these tasks are not representative of real optimization problems in drug discovery. Inspired by problems faced during medicinal chemistry lead optimization, we extend our model with multi-objective reinforcement learning, which maximizes drug-likeness while maintaining similarity to the original molecule. We further show the path through chemical space to achieve optimization for a molecule to understand how the model works.},
archivePrefix = {arXiv},
arxivId = {1810.08678},
author = {Zhou, Zhenpeng and Kearnes, Steven and Li, Li and Zare, Richard N. and Riley, Patrick},
doi = {10.1038/s41598-019-47148-x},
eprint = {1810.08678},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2019 - Zhou et al. - Optimization of Molecules via Deep Reinforcement Learning.pdf:pdf},
issn = {20452322},
journal = {Sci. Rep.},
title = {{Optimization of Molecules via Deep Reinforcement Learning}},
year = {2019}
}
@article{Kuzmin2020,
abstract = {Cluster perturbation theory is applied to the two-dimensional Hubbard t-t′-t′′-U model to obtain doping and temperature-dependent electronic spectral function with 4×4 and 12-site clusters. It is shown that evolution of the pseudogap and electronic dispersion with doping and temperature is similar and in both cases it is significantly influenced by spin-spin short-range correlations. When short-range magnetic order is weakened by doping or temperature and Hubbard-I-like electronic dispersion becomes more pronounced, the Fermi arc turns into a large Fermi surface and the pseudogap closes. It is demonstrated how static spin correlations impact the overall dispersion's shape and how accounting for dynamic contributions leads to momentum-dependent spectral weight at the Fermi surface and broadening effects.},
archivePrefix = {arXiv},
arxivId = {2001.05143},
author = {Kuz'min, V. I. and Visotin, M. A. and Nikolaev, S. V. and Ovchinnikov, S. G.},
doi = {10.1103/PhysRevB.101.115141},
eprint = {2001.05143},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2020 - Kuz'min et al. - Doping and temperature evolution of pseudogap and spin-spin correlations in the two-dimensional Hubbard model.pdf:pdf},
issn = {24699969},
journal = {Phys. Rev. B},
keywords = {doi:10.1103/PhysRevB.101.115141 url:https://doi.or},
number = {11},
pages = {1--12},
publisher = {American Physical Society},
title = {{Doping and temperature evolution of pseudogap and spin-spin correlations in the two-dimensional Hubbard model}},
volume = {101},
year = {2020}
}
@article{Balle2019,
annote = {Maude's Reading list:
https://docs.google.com/document/d/1YR2WmtKgQnQRDq44M0rr5f9OAS5Y6jzSTCYECI8YAvQ/edit?pli=1{\#}},
author = {Balle, Borja and Rabusseau, Guillaume},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2019 - Balle, Rabusseau - Approximate Minimization of Weighted Tree Automata.pdf:pdf},
pages = {1--27},
title = {{Approximate Minimization of Weighted Tree Automata}},
url = {https://www-labs.iro.umontreal.ca/{~}grabus/files/IandC2019.pdf},
year = {2019}
}
@article{Schmidt2009a,
author = {Schmidt, Michael and Lipson, Hod},
doi = {10.1126/science.1165893},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2009 - Schmidt, Lipson - Distilling Natural Laws.pdf:pdf},
journal = {Science (80-. ).},
number = {April},
pages = {81--85},
title = {{Distilling Natural Laws}},
volume = {324},
year = {2009}
}
@article{Abel2017,
abstract = {ConspectusA principal goal of drug discovery project is to design molecules that can tightly and selectively bind to the target protein receptor. Accurate prediction of protein-ligand binding free energies is therefore of central importance in computational chemistry and computer aided drug design. Multiple recent improvements in computing power, classical force field accuracy, enhanced sampling methods, and simulation setup have enabled accurate and reliable calculations of protein-ligands binding free energies, and position free energy calculations to play a guiding role in small molecule drug discovery. In this Account, we outline the relevant methodological advances, including the REST2 (Replica Exchange with Solute Temperting) enhanced sampling, the incorporation of REST2 sampling with convential FEP (Free Energy Perturbation) through FEP/REST, the OPLS3 force field, and the advanced simulation setup that constitute our FEP+ approach, followed by the presentation of extensive comparisons with experiment, demonstrating sufficient accuracy in potency prediction (better than 1 kcal/mol) to substantially impact lead optimization campaigns. The limitations of the current FEP+ implementation and best practices in drug discovery applications are also discussed followed by the future methodology development plans to address those limitations. We then report results from a recent drug discovery project, in which several thousand FEP+ calculations were successfully deployed to simultaneously optimize potency, selectivity, and solubility, illustrating the power of the approach to solve challenging drug design problems.The capabilities of free energy calculations to accurately predict potency and selectivity have led to the advance of ongoing drug discovery projects, in challenging situations where alternative approaches would have great difficulties. The ability to effectively carry out projects evaluating tens of thousands, or hundreds of thousands, of proposed drug candidates, is potentially transformative in enabling hard to drug targets to be attacked, and in facilitating the development of superior compounds, in various dimensions, for a wide range of targets. More effective integration of FEP+ calculations into the drug discovery process will ensure that the results are deployed in an optimal fashion for yielding the best possible compounds entering the clinic; this is where the greatest payoff is in the exploitation of computer driven design capabilities.A key conclusion from the work described is the surprisingly robust and accurate results that are attainable within the conventional classical simulation, fixed charge paradigm. No doubt there are individual cases that would benefit from a more sophisticated energy model or dynamical treatment, and properties other than protein-ligand binding energies may be more sensitive to these approximations. We conclude that an inflection point in the ability of MD simulations to impact drug discovery has now been attained, due to the confluence of hardware and software development along with the formulation of "good enough" theoretical methods and models.},
author = {Abel, Robert and Wang, Lingle and Harder, Edward D. and Berne, B. J. and Friesner, Richard A.},
doi = {10.1021/acs.accounts.7b00083},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2017 - Abel et al. - Advancing Drug Discovery through Enhanced Free Energy Calculations.pdf:pdf},
issn = {15204898},
journal = {Acc. Chem. Res.},
month = {jul},
number = {7},
pages = {1625--1632},
publisher = {American Chemical Society},
title = {{Advancing Drug Discovery through Enhanced Free Energy Calculations}},
volume = {50},
year = {2017}
}
@article{Rabusseau2020,
abstract = {In this paper, we unravel a fundamental connection between weighted finite automata (WFAs) and second-order recurrent neural networks (2-RNNs): in the case of sequences of discrete symbols, WFAs and 2-RNNs with linear activation functions are expressively equivalent. Motivated by this result, we build upon a recent extension of the spectral learning algorithm to vector-valued WFAs and propose the first provable learning algorithm for linear 2-RNNs defined over sequences of continuous input vectors. This algorithm relies on estimating low rank sub-blocks of the so-called Hankel tensor, from which the parameters of a linear 2-RNN can be provably recovered. The performances of the proposed method are assessed in a simulation study.},
archivePrefix = {arXiv},
arxivId = {1807.01406},
author = {Rabusseau, Guillaume and Precup, Doina},
eprint = {1807.01406},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2020 - Rabusseau, Precup - Connecting weighted automata and recurrent neural networks through spectral learning.pdf:pdf},
journal = {AISTATS 2019 - 22nd Int. Conf. Artif. Intell. Stat.},
keywords = {corresponding author},
title = {{Connecting weighted automata and recurrent neural networks through spectral learning}},
year = {2020}
}
@article{Math2014,
abstract = {This paper examines the various mathematical representations and approximations of functions and 2D curves. Using Bernstein basis polynomials, we can approximate the core ingredients that make up a function and refine the approximation to any desired precision. By the Weierstrass Approximation Theorem, we can prove that these Bernstein polynomials do in fact converge to the function we input. Remarkably, it does not take many Bernstein polynomials combined together to achieve fair approximations of most functions. Lastly, we describe how B{\'{e}}zier curves, which also use Bernstein polynomials, can also be used to describe parameterized curves in 2D space and how their representation is advantageous and well suited for uses in CAD software. Along the way we provide thorough definitions and useful applications of each topic and illustrations that are vital to understanding this highly visual topic.},
author = {Math, Grant Timmerman},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2014 - Math - Approximating Continuous Functions and Curves using Bernstein Polynomials.pdf:pdf},
journal = {Contin. Funct. Curves},
keywords = {Bernstein polynomials,B{\'{e}}zier curves,Computer Aided Design,Control Points,Curves,Line interpolation,Weierstrass Approximation Theorem},
number = {3},
pages = {1--14},
title = {{Approximating Continuous Functions and Curves using Bernstein Polynomials}},
volume = {5},
year = {2014}
}
@article{Gomez-Bombarelli2018a,
abstract = {We report a method to convert discrete representations of molecules to and from a multidimensional continuous representation. This model allows us to generate new molecules for efficient exploration and optimization through open-ended spaces of chemical compounds. A deep neural network was trained on hundreds of thousands of existing chemical structures to construct three coupled functions: an encoder, a decoder, and a predictor. The encoder converts the discrete representation of a molecule into a real-valued continuous vector, and the decoder converts these continuous vectors back to discrete molecular representations. The predictor estimates chemical properties from the latent continuous vector representation of the molecule. Continuous representations of molecules allow us to automatically generate novel chemical structures by performing simple operations in the latent space, such as decoding random vectors, perturbing known chemical structures, or interpolating between molecules. Continuous representations also allow the use of powerful gradient-based optimization to efficiently guide the search for optimized functional compounds. We demonstrate our method in the domain of drug-like molecules and also in a set of molecules with fewer that nine heavy atoms.},
annote = {Variational auto encoder on SMILES with basic mlp},
archivePrefix = {arXiv},
arxivId = {1610.02415},
author = {G{\'{o}}mez-Bombarelli, Rafael and Wei, Jennifer N. and Duvenaud, David and Hern{\'{a}}ndez-Lobato, Jos{\'{e}} Miguel and S{\'{a}}nchez-Lengeling, Benjam{\'{i}}n and Sheberla, Dennis and Aguilera-Iparraguirre, Jorge and Hirzel, Timothy D. and Adams, Ryan P. and Aspuru-Guzik, Al{\'{a}}n},
doi = {10.1021/acscentsci.7b00572},
eprint = {1610.02415},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2018 - G{\'{o}}mez-Bombarelli et al. - Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules.pdf:pdf},
issn = {23747951},
journal = {ACS Cent. Sci.},
month = {feb},
number = {2},
pages = {268--276},
publisher = {American Chemical Society},
title = {{Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules}},
volume = {4},
year = {2018}
}
@inproceedings{Jin2018d,
abstract = {We seek to automate the design of molecules based on specific chemical properties. In computational terms, this task involves continuous embedding and generation of molecular graphs. Our primary contribution is the direct realization of molecular graphs, a task previously approached by generating linear SMILES strings instead of graphs. Our junction tree variational autoencoder generates molecular graphs in two phases, by first generating a tree-structured scaffold over chemical substructures, and then combining them into a molecule with a graph message passing network. This approach allows us to incrementally expand molecules while maintaining chemical validity at every step. We evaluate our model on multiple tasks ranging from molecular generation to optimization. Across these tasks, our model outperforms previous state-of-the-art baselines by a significant margin.},
annote = {Junction Tree Variational Auto-encoder},
archivePrefix = {arXiv},
arxivId = {1802.04364},
author = {Jin, Wengong and Barzilay, Regina and Jaakkola, Tbmmi},
booktitle = {35th Int. Conf. Mach. Learn. ICML 2018},
eprint = {1802.04364},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2018 - Jin, Barzilay, Jaakkola - Junction tree variational autoencoder for molecular graph generation.pdf:pdf},
isbn = {9781510867963},
pages = {3632--3648},
title = {{Junction tree variational autoencoder for molecular graph generation}},
volume = {5},
year = {2018}
}
@article{Verret2017,
abstract = {Since its experimental discovery, many phenomenological theories successfully reproduced the rapid rise of the Hall number nH, going from p at low doping to 1+p at the critical doping p∗ of the pseudogap in superconducting cuprates. Further comparison with experiments is now needed in order to narrow down candidates. In this paper, we consider three previously successful phenomenological theories in a unified formalism - an antiferromagnetic mean field (AF), a spiral incommensurate antiferromagnetic mean field (sAF), and the Yang-Rice-Zhang (YRZ) theory. We find a rapid rise in the specific heat and a rapid drop in the Seebeck coefficient for increasing doping across the transition in each of those models. The predicted rises and drops are locked, not to p∗, but to the doping where antinodal electron pockets, characteristic of each model, appear at the Fermi surface shortly before p∗. While such electron pockets are still to be found in experiments, we discuss how they could provide distinctive signatures for each model. We also show that the range of doping where those electron pockets would be found is strongly affected by the position of the van Hove singularity.},
archivePrefix = {arXiv},
arxivId = {1707.04632},
author = {Verret, S. and Simard, O. and Charlebois, M. and S{\'{e}}n{\'{e}}chal, D. and Tremblay, A. M.S.},
doi = {10.1103/PhysRevB.96.125139},
eprint = {1707.04632},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2017 - Verret et al. - Phenomenological theories of the low-temperature pseudogap Hall number, specific heat, and Seebeck coefficient.pdf:pdf},
issn = {24699969},
journal = {Phys. Rev. B},
number = {12},
pages = {1--11},
title = {{Phenomenological theories of the low-temperature pseudogap: Hall number, specific heat, and Seebeck coefficient}},
volume = {96},
year = {2017}
}
@article{Macek2020,
abstract = {Understanding the quantum many-body problem and its applications to correlated materials, cold atoms and nanoelectronic devices is a central problem of physics. Nonetheless, few numerical techniques can simulate strongly correlated systems in an accurate and controlled way, especially when far from equilibrium. Perturbation theory has seen an unexpected recent revival, based on Quantum Monte Carlo approaches that calculate all Feynman diagrams up to large orders. Here we show that integration based on low-discrepancy sequences can be adapted to this problem and greatly outperforms state-of-the-art diagrammatic Monte Carlo methods. In relevant practical applications, we show a speed-up of several orders of magnitude. We demonstrate convergence with scaling as fast as {\$}1/N{\$} in the number of sample points {\$}N{\$}, parametrically faster than the {\$}1/\backslashsqrt{\{}N{\}}{\$} of Monte Carlo methods. Our approach enables a new scale of high-precision computation for strongly interacting quantum many-body systems. We illustrate it with a solution of the Kondo ridge in quantum dots.},
archivePrefix = {arXiv},
arxivId = {2002.12372},
author = {Ma{\v{c}}ek, Marjan and Dumitrescu, Philipp T. and Bertrand, Corentin and Triggs, Bill and Parcollet, Olivier and Waintal, Xavier},
eprint = {2002.12372},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2020 - Ma{\v{c}}ek et al. - Quantum Quasi-Monte Carlo.pdf:pdf},
pages = {1--14},
title = {{Quantum Quasi-Monte Carlo}},
url = {http://arxiv.org/abs/2002.12372},
year = {2020}
}
@article{Kades2019,
abstract = {We explore artificial neural networks as a tool for the reconstruction of spectral functions from imaginary time Green's functions, a classic ill-conditioned inverse problem. Our ansatz is based on a supervised learning framework in which prior knowledge is encoded in the training data and the inverse transformation manifold is explicitly parametrised through a neural network. We systematically investigate this novel reconstruction approach, providing a detailed analysis of its performance on physically motivated mock data, and compare it to established methods of Bayesian inference. The reconstruction accuracy is found to be at least comparable, and potentially superior in particular at larger noise levels. We argue that the use of labelled training data in a supervised setting and the freedom in defining an optimisation objective are inherent advantages of the present approach and may lead to significant improvements over state-of-the-art methods in the future. Potential directions for further research are discussed in detail.},
archivePrefix = {arXiv},
arxivId = {1905.04305},
author = {Kades, Lukas and Pawlowski, Jan M. and Rothkopf, Alexander and Scherzer, Manuel and Urban, Julian M. and Wetzel, Sebastian J. and Wink, Nicolas and Ziegler, Felix},
eprint = {1905.04305},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2019 - Kades et al. - Spectral Reconstruction with Deep Neural Networks.pdf:pdf},
pages = {1--18},
title = {{Spectral Reconstruction with Deep Neural Networks}},
url = {http://arxiv.org/abs/1905.04305},
year = {2019}
}
@article{Nielsen2020a,
abstract = {We define a class of tensor network states for spin systems where the individual tensors are functionals of fields. The construction is based on the path integral representation of correlators of operators in quantum field theory. These tensor network states are infinite dimensional versions of matrix product states and projected entangled pair states. We find the field-tensor that generates the Haldane-Shastry wave function and extend it to two dimensions. We give evidence that the latter underlies the topological chiral state described by the Kalmeyer-Laughlin wave function.},
archivePrefix = {arXiv},
arxivId = {2001.07723},
author = {Nielsen, Anne E. B. and Herwerth, Benedikt and Cirac, J. Ignacio and Sierra, Germ{\'{a}}n},
eprint = {2001.07723},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2020 - Nielsen et al. - Field Tensor Network States(2).pdf:pdf},
title = {{Field Tensor Network States}},
url = {http://arxiv.org/abs/2001.07723},
year = {2020}
}
@article{Schrittwieser2019,
abstract = {Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.},
archivePrefix = {arXiv},
arxivId = {1911.08265},
author = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and Lillicrap, Timothy and Silver, David},
eprint = {1911.08265},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2019 - Schrittwieser et al. - Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model.pdf:pdf},
pages = {1--21},
title = {{Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model}},
url = {http://arxiv.org/abs/1911.08265},
year = {2019}
}
@article{Verdon2019a,
abstract = {Quantum Neural Networks (QNNs) are a promising variational learning paradigm with applications to near-term quantum processors, however they still face some significant challenges. One such challenge is finding good parameter initialization heuristics that ensure rapid and consistent convergence to local minima of the parameterized quantum circuit landscape. In this work, we train classical neural networks to assist in the quantum learning process, also know as meta-learning, to rapidly find approximate optima in the parameter landscape for several classes of quantum variational algorithms. Specifically, we train classical recurrent neural networks to find approximately optimal parameters within a small number of queries of the cost function for the Quantum Approximate Optimization Algorithm (QAOA) for MaxCut, QAOA for Sherrington-Kirkpatrick Ising model, and for a Variational Quantum Eigensolver for the Hubbard model. By initializing other optimizers at parameter values suggested by the classical neural network, we demonstrate a significant improvement in the total number of optimization iterations required to reach a given accuracy. We further demonstrate that the optimization strategies learned by the neural network generalize well across a range of problem instance sizes. This opens up the possibility of training on small, classically simulatable problem instances, in order to initialize larger, classically intractably simulatable problem instances on quantum devices, thereby significantly reducing the number of required quantum-classical optimization iterations.},
archivePrefix = {arXiv},
arxivId = {1907.05415},
author = {Verdon, Guillaume and Broughton, Michael and McClean, Jarrod R. and Sung, Kevin J. and Babbush, Ryan and Jiang, Zhang and Neven, Hartmut and Mohseni, Masoud},
eprint = {1907.05415},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2019 - Verdon et al. - Learning to learn with quantum neural networks via classical neural networks.pdf:pdf},
pages = {1--12},
title = {{Learning to learn with quantum neural networks via classical neural networks}},
url = {http://arxiv.org/abs/1907.05415},
year = {2019}
}
@article{Jin2017,
abstract = {The prediction of organic reaction outcomes is a fundamental problem in computational chemistry. Since a reaction may involve hundreds of atoms, fully exploring the space of possible transformations is intractable. The current solution utilizes reaction templates to limit the space, but it suffers from coverage and efficiency issues. In this paper, we propose a template-free approach to efficiently explore the space of product molecules by first pinpointing the reaction center - the set of nodes and edges where graph edits occur. Since only a small number of atoms contribute to reaction center, we can directly enumerate candidate products. The generated candidates are scored by a Weisfeiler-Lehman Difference Network that models high-order interactions between changes occurring at nodes across the molecule. Our framework outperforms the top-performing template-based approach with a 10{\%} margin, while running orders of magnitude faster. Finally, we demonstrate that the model accuracy rivals the performance of domain experts.},
archivePrefix = {arXiv},
arxivId = {1709.04555},
author = {Jin, Wengong and Coley, Connor W. and Barzilay, Regina and Jaakkola, Tommi},
eprint = {1709.04555},
file = {:Users/Simon/Documents/papiers{\_}mendeley//2017 - Jin et al. - Predicting organic reaction outcomes with weisfeiler-lehman network.pdf:pdf},
issn = {10495258},
journal = {Adv. Neural Inf. Process. Syst.},
number = {Nips},
pages = {2608--2617},
title = {{Predicting organic reaction outcomes with weisfeiler-lehman network}},
volume = {2017-Decem},
year = {2017}
}
@article{Misra,
abstract = {We propose {\$}\backslashtextit{\{}Mish{\}}{\$}, a novel self-regularized non-monotonic activation function which can be mathematically defined as: {\$}f(x)=x\backslashtanh(softplus(x)){\$}. As activation functions play a crucial role in the performance and training dynamics in neural networks, we validated experimentally on several well-known benchmarks against the best combinations of architectures and activation functions. We also observe that data augmentation techniques have a favorable effect on benchmarks like ImageNet-1k and MS-COCO across multiple architectures. For example, Mish outperformed Leaky ReLU on YOLOv4 with a CSP-DarkNet-53 backbone on average precision ({\$}AP{\_}{\{}50{\}}{\^{}}{\{}val{\}}{\$}) by 2.1{\$}\backslash{\%}{\$} in MS-COCO object detection and ReLU on ResNet-50 on ImageNet-1k in Top-1 accuracy by {\$}\backslashapprox{\$}1{\$}\backslash{\%}{\$} while keeping all other network parameters and hyperparameters constant. Furthermore, we explore the mathematical formulation of Mish in relation with the Swish family of functions and propose an intuitive understanding on how the first derivative behavior may be acting as a regularizer helping the optimization of deep neural networks. Code is publicly available at https://github.com/digantamisra98/Mish.},
archivePrefix = {arXiv},
arxivId = {1908.08681},
author = {Misra, Diganta},
eprint = {1908.08681},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2019 - Misra - Mish A Self Regularized Non-Monotonic Activation Function.pdf:pdf},
journal = {Mish A Self Regul. Non-Monotonic Neural Act. Funct.},
month = {aug},
number = {1},
title = {{Mish: A Self Regularized Non-Monotonic Activation Function}},
url = {http://arxiv.org/abs/1908.08681},
year = {2019}
}
@article{Campos2019a,
abstract = {We compute the partition function of a massive free boson in a square lattice using a tensor network algorithm. We introduce a singular value decomposition of continuous matrices that leads to very accurate numerical results. It shows the emergence of a corner double line fixed-point structure. In the massless limit, we reproduce the results of conformal field theory including a precise value of the central charge.},
archivePrefix = {arXiv},
arxivId = {1902.02362},
author = {Campos, Manuel and Sierra, Germ{\'{a}}n and L{\'{o}}pez, Esperanza},
doi = {10.1103/PhysRevB.100.195106},
eprint = {1902.02362},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2019 - Campos, Sierra, L{\'{o}}pez - Tensor renormalization group in bosonic field theory(2).pdf:pdf},
issn = {24699969},
journal = {Phys. Rev. B},
keywords = {doi:10.1103/PhysRevB.100.195106 url:https://doi.or},
number = {19},
pages = {1--11},
publisher = {American Physical Society},
title = {{Tensor renormalization group in bosonic field theory}},
volume = {100},
year = {2019}
}
@inproceedings{Nakkiran2020Deep,
abstract = {We show that a variety of modern deep learning tasks exhibit a "double-descent" phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.},
archivePrefix = {arXiv},
arxivId = {1912.02292},
author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
booktitle = {Int. Conf. Learn. Represent.},
eprint = {1912.02292},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2020 - Nakkiran et al. - Deep Double Descent Where Bigger Models and More Data Hurt.pdf:pdf},
month = {dec},
title = {{Deep Double Descent: Where Bigger Models and More Data Hurt}},
url = {https://openreview.net/forum?id=B1g5sA4twr http://arxiv.org/abs/1912.02292},
year = {2020}
}
@article{Delage2007,
abstract = {Markov decision processes are an effective tool in modeling decision-making in uncertain dynamic environments. Since the parameters of these models are typically estimated from data, learned from experience, or designed by hand, it is not surprising that the actual performance of a chosen strategy often significantly differs from the designer's initial expectations due to unavoidable model uncertainty. In this paper, we present a percentile criterion that captures the trade-off between optimistic and pessimistic points of view on MDP with parameter uncertainty. We describe tractable methods that take parameter uncertainty into account in the process of decision making. Finally, we propose a cost-effective exploration strategy when it is possible to invest (money, time or computation efforts) in actions that will reduce the uncertainty in the parameters.},
author = {Delage, Erick and Mannor, Shie},
doi = {10.1145/1273496.1273525},
file = {::},
journal = {ACM Int. Conf. Proceeding Ser.},
pages = {225--232},
title = {{Percentile optimization in uncertain Markov decision processes with application to efficient exploration}},
volume = {227},
year = {2007}
}
@article{Schutt2019,
abstract = {SchNetPack is a toolbox for the development and application of deep neural networks that predict potential energy surfaces and other quantum-chemical properties of molecules and materials. It contains basic building blocks of atomistic neural networks, manages their training, and provides simple access to common benchmark datasets. This allows for an easy implementation and evaluation of new models. For now, SchNetPack includes implementations of (weighted) atom-centered symmetry functions and the deep tensor neural network SchNet, as well as ready-to-use scripts that allow one to train these models on molecule and material datasets. Based on the PyTorch deep learning framework, SchNetPack allows one to efficiently apply the neural networks to large datasets with millions of reference calculations, as well as parallelize the model across multiple GPUs. Finally, SchNetPack provides an interface to the Atomic Simulation Environment in order to make trained models easily accessible to researchers that are not yet familiar with neural networks.},
archivePrefix = {arXiv},
arxivId = {1809.01072},
author = {Sch{\"{u}}tt, K. T. and Kessel, P. and Gastegger, M. and Nicoli, K. A. and Tkatchenko, A. and M{\"{u}}ller, K. R.},
doi = {10.1021/acs.jctc.8b00908},
eprint = {1809.01072},
file = {::},
issn = {15499626},
journal = {J. Chem. Theory Comput.},
number = {1},
pages = {448--455},
title = {{SchNetPack: A Deep Learning Toolbox for Atomistic Systems}},
volume = {15},
year = {2019}
}
@article{Jin2017,
abstract = {The prediction of organic reaction outcomes is a fundamental problem in computational chemistry. Since a reaction may involve hundreds of atoms, fully exploring the space of possible transformations is intractable. The current solution utilizes reaction templates to limit the space, but it suffers from coverage and efficiency issues. In this paper, we propose a template-free approach to efficiently explore the space of product molecules by first pinpointing the reaction center - the set of nodes and edges where graph edits occur. Since only a small number of atoms contribute to reaction center, we can directly enumerate candidate products. The generated candidates are scored by a Weisfeiler-Lehman Difference Network that models high-order interactions between changes occurring at nodes across the molecule. Our framework outperforms the top-performing template-based approach with a 10{\%} margin, while running orders of magnitude faster. Finally, we demonstrate that the model accuracy rivals the performance of domain experts.},
archivePrefix = {arXiv},
arxivId = {1709.04555},
author = {Jin, Wengong and Coley, Connor W. and Barzilay, Regina and Jaakkola, Tommi},
eprint = {1709.04555},
file = {:Users/Simon/Documents/papiers{\_}mendeley//2017 - Jin et al. - Predicting organic reaction outcomes with weisfeiler-lehman network.pdf:pdf},
issn = {10495258},
journal = {Adv. Neural Inf. Process. Syst.},
number = {Nips},
pages = {2608--2617},
title = {{Predicting organic reaction outcomes with weisfeiler-lehman network}},
volume = {2017-Decem},
year = {2017}
}
@article{Jaderberg2017,
abstract = {Neural networks dominate the modern machine learning landscape, but their training and success still suffer from sensitivity to empirical choices of hyperparameters such as model architecture, loss function, and optimisation algorithm. In this work we present $\backslash$emph{\{}Population Based Training (PBT){\}}, a simple asynchronous optimisation algorithm which effectively utilises a fixed computational budget to jointly optimise a population of models and their hyperparameters to maximise performance. Importantly, PBT discovers a schedule of hyperparameter settings rather than following the generally sub-optimal strategy of trying to find a single fixed set to use for the whole course of training. With just a small modification to a typical distributed hyperparameter training framework, our method allows robust and reliable training of models. We demonstrate the effectiveness of PBT on deep reinforcement learning problems, showing faster wall-clock convergence and higher final performance of agents by optimising over a suite of hyperparameters. In addition, we show the same method can be applied to supervised learning for machine translation, where PBT is used to maximise the BLEU score directly, and also to training of Generative Adversarial Networks to maximise the Inception score of generated images. In all cases PBT results in the automatic discovery of hyperparameter schedules and model selection which results in stable training and better final performance.},
archivePrefix = {arXiv},
arxivId = {1711.09846},
author = {Jaderberg, Max and Dalibard, Valentin and Osindero, Simon and Czarnecki, Wojciech M. and Donahue, Jeff and Razavi, Ali and Vinyals, Oriol and Green, Tim and Dunning, Iain and Simonyan, Karen and Fernando, Chrisantha and Kavukcuoglu, Koray},
eprint = {1711.09846},
file = {::},
title = {{Population Based Training of Neural Networks}},
url = {http://arxiv.org/abs/1711.09846},
year = {2017}
}
@article{Flam-Shepherd2020,
abstract = {Graph neural network have achieved impressive results in predicting molecular properties, but they do not directly account for local and hidden structures in the graph such as functional groups and molecular geometry. At each propagation step, GNNs aggregate only over first order neighbours, ignoring important information contained in subsequent neighbours as well as the relationships between those higher order connections. In this work, we generalize graph neural nets to pass messages and aggregate across higher order paths. This allows for information to propagate over various levels and substructures of the graph. We demonstrate our model on a few tasks in molecular property prediction.},
archivePrefix = {arXiv},
arxivId = {2002.10413},
author = {Flam-Shepherd, Daniel and Wu, Tony and Friederich, Pascal and Aspuru-Guzik, Alan},
eprint = {2002.10413},
file = {::},
pages = {1--7},
title = {{Neural Message Passing on High Order Paths}},
url = {http://arxiv.org/abs/2002.10413},
year = {2020}
}
@article{Jin2020a,
abstract = {Graph generation techniques are increasingly being adopted for drug discovery. Previous graph generation approaches have utilized relatively small molecular building blocks such as atoms or simple cycles, limiting their effectiveness to smaller molecules. Indeed, as we demonstrate, their performance degrades significantly for larger molecules. In this paper, we propose a new hierarchical graph encoder-decoder that employs significantly larger and more flexible graph motifs as basic building blocks. Our encoder produces a multi-resolution representation for each molecule in a fine-to-coarse fashion, from atoms to connected motifs. Each level integrates the encoding of constituents below with the graph at that level. Our autoregressive coarse-to-fine decoder adds one motif at a time, interleaving the decision of selecting a new motif with the process of resolving its attachments to the emerging molecule. We evaluate our model on multiple molecule generation tasks, including polymers, and show that our model significantly outperforms previous state-of-the-art baselines.},
archivePrefix = {arXiv},
arxivId = {2002.03230},
author = {Jin, Wengong and Barzilay, Regina and Jaakkola, Tommi},
eprint = {2002.03230},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2020 - Jin, Barzilay, Jaakkola - Hierarchical Generation of Molecular Graphs using Structural Motifs.pdf:pdf;::},
title = {{Hierarchical Generation of Molecular Graphs using Structural Motifs}},
url = {http://arxiv.org/abs/2002.03230},
year = {2020}
}
@article{Jin2018,
abstract = {We seek to automate the design of molecules based on specific chemical properties. In computational terms, this task involves continuous embedding and generation of molecular graphs. Our primary contribution is the direct realization of molecular graphs, a task previously approached by generating linear SMILES strings instead of graphs. Our junction tree variational autoencoder generates molecular graphs in two phases, by first generating a tree-structured scaffold over chemical substructures, and then combining them into a molecule with a graph message passing network. This approach allows us to incrementally expand molecules while maintaining chemical validity at every step. We evaluate our model on multiple tasks ranging from molecular generation to optimization. Across these tasks, our model outperforms previous state-of-the-art baselines by a significant margin.},
archivePrefix = {arXiv},
arxivId = {1802.04364},
author = {Jin, Wengong and Barzilay, Regina and Jaakkola, Tbmmi},
eprint = {1802.04364},
file = {::},
isbn = {9781510867963},
journal = {35th Int. Conf. Mach. Learn. ICML 2018},
pages = {3632--3648},
title = {{Junction tree variational autoencoder for molecular graph generation}},
volume = {5},
year = {2018}
}
@article{Wang2019,
author = {Wang, Yue and Solomon, Justin},
file = {::},
number = {NeurIPS},
title = {{PRNet : Self-Supervised Learning for Partial-to-Partial Registration}},
year = {2019}
}
@article{Imrie2020,
abstract = {Rational compound design remains a challenging problem for both computational methods and medicinal chemists. Computational generative methods have begun to show promising results for the design problem. However, they have not yet used the power of three-dimensional (3D) structural information. We have developed a novel graph-based deep generative model that combines state-of-the-art machine learning techniques with structural knowledge. Our method ("DeLinker") takes two fragments or partial structures and designs a molecule incorporating both. The generation process is protein-context-dependent, utilizing the relative distance and orientation between the partial structures. This 3D information is vital to successful compound design, and we demonstrate its impact on the generation process and the limitations of omitting such information. In a large-scale evaluation, DeLinker designed 60{\%} more molecules with high 3D similarity to the original molecule than a database baseline. When considering the more relevant problem of longer linkers with at least five atoms, the outperformance increased to 200{\%}. We demonstrate the effectiveness and applicability of this approach on a diverse range of design problems: fragment linking, scaffold hopping, and proteolysis targeting chimera (PROTAC) design. As far as we are aware, this is the first molecular generative model to incorporate 3D structural information directly in the design process. The code is available at https://github.com/oxpig/DeLinker.},
author = {Imrie, Fergus and Bradley, Anthony R. and van der Schaar, Mihaela and Deane, Charlotte M.},
doi = {10.1021/acs.jcim.9b01120},
file = {::},
issn = {1549960X},
journal = {J. Chem. Inf. Model.},
number = {4},
pages = {1983--1995},
pmid = {32195587},
title = {{Deep Generative Models for 3D Linker Design}},
volume = {60},
year = {2020}
}
@article{Korablyov2019,
author = {Korablyov, Maksym and Goyal, Anirudh and Chiu, Erica and Bronstein, Michael and Bengio, Yoshua},
file = {:Users/Simon/Documents/papiers{\_}mendeley//2019 - Korablyov et al. - Fragment-based drug design with Small Prior Neural Networks.pdf:pdf},
number = {NeurIPS},
title = {{Fragment-based drug design with Small Prior Neural Networks}},
year = {2019}
}
@article{Hu2019,
abstract = {Many applications of machine learning require a model to make accurate pre-dictions on test examples that are distributionally different from training ones, while task-specific labels are scarce during training. An effective approach to this challenge is to pre-train a model on related tasks where data is abundant, and then fine-tune it on a downstream task of interest. While pre-training has been effective in many language and vision domains, it remains an open question how to effectively use pre-training on graph datasets. In this paper, we develop a new strategy and self-supervised methods for pre-training Graph Neural Networks (GNNs). The key to the success of our strategy is to pre-train an expressive GNN at the level of individual nodes as well as entire graphs so that the GNN can learn useful local and global representations simultaneously. We systematically study pre-training on multiple graph classification datasets. We find that naive strategies, which pre-train GNNs at the level of either entire graphs or individual nodes, give limited improvement and can even lead to negative transfer on many downstream tasks. In contrast, our strategy avoids negative transfer and improves generalization significantly across downstream tasks, leading up to 9.4{\%} absolute improvements in ROC-AUC over non-pre-trained models and achieving state-of-the-art performance for molecular property prediction and protein function prediction.},
archivePrefix = {arXiv},
arxivId = {1905.12265},
author = {Hu, Weihua and Liu, Bowen and Gomes, Joseph and Zitnik, Marinka and Liang, Percy and Pande, Vijay and Leskovec, Jure},
eprint = {1905.12265},
file = {::},
pages = {1--22},
title = {{Strategies for Pre-training Graph Neural Networks}},
url = {http://arxiv.org/abs/1905.12265},
year = {2019}
}
@article{Bose2019,
abstract = {Learning high-quality node embeddings is a key building block for machine learning models that operate on graph data, such as social networks and recommender systems. However, existing graph embedding techniques are unable to cope with fairness constraints, e.g., ensuring that the learned representations do not correlate with certain attributes, such as age or gender. Here, we introduce an adversarial framework to enforce fairness constraints on graph embeddings. Our approach is compositional-meaning that it can flexibly accommodate different combinations of fairness constraints during inference. For instance, in the context of social recommendations, our framework would allow one user to request that their recommendations arc invariant to both their age and gender, while also allowing another user to request invariance to just their age. Experiments on standard knowledge graph and recommender system benchmarks highlight the utility of our proposed framework.},
archivePrefix = {arXiv},
arxivId = {1905.10674},
author = {Bose, Avishek Joey and Hamilton, William L.},
eprint = {1905.10674},
file = {::},
isbn = {9781510886988},
journal = {36th Int. Conf. Mach. Learn. ICML 2019},
pages = {1139--1148},
title = {{Compositional fairness constraints for graph embeddings}},
volume = {2019-June},
year = {2019}
}
@article{Brown2019,
abstract = {De novo design seeks to generate molecules with required property profiles by virtual design-make-test cycles. With the emergence of deep learning and neural generative models in many application areas, models for molecular design based on neural networks appeared recently and show promising results. However, the new models have not been profiled on consistent tasks, and comparative studies to well-established algorithms have only seldom been performed. To standardize the assessment of both classical and neural models for de novo molecular design, we propose an evaluation framework, GuacaMol, based on a suite of standardized benchmarks. The benchmark tasks encompass measuring the fidelity of the models to reproduce the property distribution of the training sets, the ability to generate novel molecules, the exploration and exploitation of chemical space, and a variety of single and multiobjective optimization tasks. The benchmarking open-source Python code and a leaderboard can be found on https://benevolent.ai/guacamol.},
archivePrefix = {arXiv},
arxivId = {1811.09621},
author = {Brown, Nathan and Fiscato, Marco and Segler, Marwin H.S. and Vaucher, Alain C.},
doi = {10.1021/acs.jcim.8b00839},
eprint = {1811.09621},
file = {::},
issn = {15205142},
journal = {J. Chem. Inf. Model.},
number = {3},
pages = {1096--1108},
title = {{GuacaMol: Benchmarking Models for de Novo Molecular Design}},
volume = {59},
year = {2019}
}
@article{Lyu2019,
abstract = {Despite intense interest in expanding chemical space, libraries containing hundreds-of-millions to billions of diverse molecules have remained inaccessible. Here we investigate structure-based docking of 170 million make-on-demand compounds from 130 well-characterized reactions. The resulting library is diverse, representing over 10.7 million scaffolds that are otherwise unavailable. For each compound in the library, docking against AmpC $\beta$-lactamase (AmpC) and the D 4 dopamine receptor were simulated. From the top-ranking molecules, 44 and 549 compounds were synthesized and tested for interactions with AmpC and the D 4 dopamine receptor, respectively. We found a phenolate inhibitor of AmpC, which revealed a group of inhibitors without known precedent. This molecule was optimized to 77 nM, which places it among the most potent non-covalent AmpC inhibitors known. Crystal structures of this and other AmpC inhibitors confirmed the docking predictions. Against the D 4 dopamine receptor, hit rates fell almost monotonically with docking score, and a hit-rate versus score curve predicted that the library contained 453,000 ligands for the D 4 dopamine receptor. Of 81 new chemotypes discovered, 30 showed submicromolar activity, including a 180-pM subtype-selective agonist of the D 4 dopamine receptor.},
author = {Lyu, Jiankun and Wang, Sheng and Balius, Trent E. and Singh, Isha and Levit, Anat and Moroz, Yurii S. and O'Meara, Matthew J. and Che, Tao and Algaa, Enkhjargal and Tolmachova, Kateryna and Tolmachev, Andrey A. and Shoichet, Brian K. and Roth, Bryan L. and Irwin, John J.},
doi = {10.1038/s41586-019-0917-9},
file = {::},
issn = {14764687},
journal = {Nature},
number = {7743},
pages = {224--229},
title = {{Ultra-large library docking for discovering new chemotypes}},
volume = {566},
year = {2019}
}
@article{Wallach2015,
abstract = {Deep convolutional neural networks comprise a subclass of deep neural networks (DNN) with a constrained architecture that leverages the spatial and temporal structure of the domain they model. Convolutional networks achieve the best predictive performance in areas such as speech and image recognition by hierarchically composing simple local features into complex models. Although DNNs have been used in drug discovery for QSAR and ligand-based bioactivity predictions, none of these models have benefited from this powerful convolutional architecture. This paper introduces AtomNet, the first structure-based, deep convolutional neural network designed to predict the bioactivity of small molecules for drug discovery applications. We demonstrate how to apply the convolutional concepts of feature locality and hierarchical composition to the modeling of bioactivity and chemical interactions. In further contrast to existing DNN techniques, we show that AtomNet's application of local convolutional filters to structural target information successfully predicts new active molecules for targets with no previously known modulators. Finally, we show that AtomNet outperforms previous docking approaches on a diverse set of benchmarks by a large margin, achieving an AUC greater than 0.9 on 57.8{\%} of the targets in the DUDE benchmark.},
archivePrefix = {arXiv},
arxivId = {1510.02855},
author = {Wallach, Izhar and Dzamba, Michael and Heifets, Abraham},
eprint = {1510.02855},
file = {::},
pages = {1--11},
title = {{AtomNet: A Deep Convolutional Neural Network for Bioactivity Prediction in Structure-based Drug Discovery}},
url = {http://arxiv.org/abs/1510.02855},
year = {2015}
}
@article{Li2016,
abstract = {Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be described as abstract data structures.},
archivePrefix = {arXiv},
arxivId = {arXiv:1511.05493v4},
author = {Li, Yujia and Zemel, Richard and Brockschmidt, Marc and Tarlow, Daniel},
eprint = {arXiv:1511.05493v4},
file = {::},
journal = {4th Int. Conf. Learn. Represent. ICLR 2016 - Conf. Track Proc.},
number = {1},
pages = {1--20},
title = {{Gated graph sequence neural networks}},
year = {2016}
}
@article{Song2018,
abstract = {How can we efficiently gather information to optimize an unknown function, when presented with multiple, mutually dependent information sources with different costs? For example, when optimizing a robotic system, intelligently trading off computer simulations and real robot testings can lead to significant savings. Existing methods, such as multi-fidelity GP-UCB or Entropy Search-based approaches, either make simplistic assumptions on the interaction among different fidelities or use simple heuristics that lack theoretical guarantees. In this paper, we study multi-fidelity Bayesian optimization with complex structural dependencies among multiple outputs, and propose MF-MI-Greedy, a principled algorithmic framework for addressing this problem. In particular, we model different fidelities using additive Gaussian processes based on shared latent structures with the target function. Then we use cost-sensitive mutual information gain for efficient Bayesian global optimization. We propose a simple notion of regret which incorporates the cost of different fidelities, and prove that MF-MI-Greedy achieves low regret. We demonstrate the strong empirical performance of our algorithm on both synthetic and real-world datasets.},
archivePrefix = {arXiv},
arxivId = {1811.00755},
author = {Song, Jialin and Chen, Yuxin and Yue, Yisong},
eprint = {1811.00755},
file = {::},
title = {{A General Framework for Multi-fidelity Bayesian Optimization with Gaussian Processes}},
url = {http://arxiv.org/abs/1811.00755},
volume = {89},
year = {2018}
}
@article{Feinberg2018,
abstract = {The arc of drug discovery entails a multiparameter optimization problem spanning vast length scales. The key parameters range from solubility (angstroms) to protein-ligand binding (nanometers) to in vivo toxicity (meters). Through feature learning - instead of feature engineering - deep neural networks promise to outperform both traditional physics-based and knowledge-based machine learning models for predicting molecular properties pertinent to drug discovery. To this end, we present the PotentialNet family of graph convolutions. These models are specifically designed for and achieve state-of-the-art performance for protein-ligand binding affinity. We further validate these deep neural networks by setting new standards of performance in several ligand-based tasks. In parallel, we introduce a new metric, the Regression Enrichment Factor EF$\chi$(R), to measure the early enrichment of computational models for chemical data. Finally, we introduce a cross-validation strategy based on structural homology clustering that can more accurately measure model generalizability, which crucially distinguishes the aims of machine learning for drug discovery from standard machine learning tasks.},
archivePrefix = {arXiv},
arxivId = {1803.04465},
author = {Feinberg, Evan N. and Sur, Debnil and Wu, Zhenqin and Husic, Brooke E. and Mai, Huanghao and Li, Yang and Sun, Saisai and Yang, Jianyi and Ramsundar, Bharath and Pande, Vijay S.},
doi = {10.1021/acscentsci.8b00507},
eprint = {1803.04465},
file = {::},
issn = {23747951},
journal = {ACS Cent. Sci.},
number = {11},
pages = {1520--1530},
pmid = {30555904},
title = {{PotentialNet for Molecular Property Prediction}},
volume = {4},
year = {2018}
}
@article{Sun2019a,
abstract = {This paper studies learning the representations of whole graphs in both unsupervised and semi-supervised scenarios. Graph-level representations are critical in a variety of real-world applications such as predicting the properties of molecules and community analysis in social networks. Traditional graph kernel based methods are simple, yet effective for obtaining fixed-length representations for graphs but they suffer from poor generalization due to hand-crafted designs. There are also some recent methods based on language models (e.g. graph2vec) but they tend to only consider certain substructures (e.g. subtrees) as graph representatives. Inspired by recent progress of unsupervised representation learning, in this paper we proposed a novel method called InfoGraph for learning graph-level representations. We maximize the mutual information between the graph-level representation and the representations of substructures of different scales (e.g., nodes, edges, triangles). By doing so, the graph-level representations encode aspects of the data that are shared across different scales of substructures. Furthermore, we further propose InfoGraph*, an extension of InfoGraph for semi-supervised scenarios. InfoGraph* maximizes the mutual information between unsupervised graph representations learned by InfoGraph and the representations learned by existing supervised methods. As a result, the supervised encoder learns from unlabeled data while preserving the latent semantic space favored by the current supervised task. Experimental results on the tasks of graph classification and molecular property prediction show that InfoGraph is superior to state-of-the-art baselines and InfoGraph* can achieve performance competitive with state-of-the-art semi-supervised models.},
archivePrefix = {arXiv},
arxivId = {1908.01000},
author = {Sun, Fan-Yun and Hoffmann, Jordan and Verma, Vikas and Tang, Jian},
eprint = {1908.01000},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2019 - Sun et al. - InfoGraph Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization.pdf:pdf},
number = {2018},
pages = {1--16},
title = {{InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization}},
url = {http://arxiv.org/abs/1908.01000},
year = {2019}
}
@article{Bellemare2016,
abstract = {We consider an agent's uncertainty about its environment and the problem of generalizing this uncertainty across states. Specifically, we focus on the problem of exploration in non-tabular reinforcement learning. Drawing inspiration from the intrinsic motivation literature, we use density models to measure uncertainty, and propose a novel algorithm for deriving a pseudo-count from an arbitrary density model. This technique enables us to generalize count-based exploration algorithms to the non-tabular case. We apply our ideas to Atari 2600 games, providing sensible pseudo-counts from raw pixels. We transform these pseudo-counts into exploration bonuses and obtain significantly improved exploration in a number of hard games, including the infamously difficult MONTEZUMA'S REVENGE.},
archivePrefix = {arXiv},
arxivId = {1606.01868},
author = {Bellemare, Marc G. and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, R{\'{e}}mi},
eprint = {1606.01868},
file = {::},
issn = {10495258},
journal = {Adv. Neural Inf. Process. Syst.},
number = {Nips},
pages = {1479--1487},
title = {{Unifying count-based exploration and intrinsic motivation}},
year = {2016}
}
@article{Liu2018,
abstract = {Graphs are ubiquitous data structures for representing interactions between entities. With an emphasis on applications in chemistry, we explore the task of learning to generate graphs that conform to a distribution observed in training data. We propose a variational autoencoder model in which both encoder and decoder are graph-structured. Our decoder assumes a sequential ordering of graph extension steps and we discuss and analyze design choices that mitigate the potential downsides of this linearization. Experiments compare our approach with a wide range of baselines on the molecule generation task and show that our method is successful at matching the statistics of the original dataset on semantically important metrics. Furthermore, we show that by using appropriate shaping of the latent space, our model allows us to design molecules that are (locally) optimal in desired properties.},
archivePrefix = {arXiv},
arxivId = {1805.09076},
author = {Liu, Qi and Allamanis, Miltiadis and Brockschmidt, Marc and Gaunt, Alexander L.},
eprint = {1805.09076},
file = {::},
issn = {10495258},
journal = {Adv. Neural Inf. Process. Syst.},
number = {NeurIPS},
pages = {7795--7804},
title = {{Constrained graph variational autoencoders for molecule design}},
volume = {2018-Decem},
year = {2018}
}
@article{You2018,
abstract = {Generating novel graph structures that optimize given objectives while obeying some given underlying rules is fundamental for chemistry, biology and social science research. This is especially important in the task of molecular graph generation, whose goal is to discover novel molecules with desired properties such as drug-likeness and synthetic accessibility, while obeying physical laws such as chemical valency. However, designing models to find molecules that optimize desired properties while incorporating highly complex and non-differentiable rules remains to be a challenging task. Here we propose Graph Convolutional Policy Network (GCPN), a general graph convolutional network based model for goal-directed graph generation through reinforcement learning. The model is trained to optimize domain-specific rewards and adversarial loss through policy gradient, and acts in an environment that incorporates domain-specific rules. Experimental results show that GCPN can achieve 61{\%} improvement on chemical property optimization over state-of-the-art baselines while resembling known molecules, and achieve 184{\%} improvement on the constrained property optimization task.},
archivePrefix = {arXiv},
arxivId = {1806.02473},
author = {You, Jiaxuan and Liu, Bowen and Ying, Rex and Pande, Vijay and Leskovec, Jure},
eprint = {1806.02473},
file = {::},
issn = {10495258},
journal = {Adv. Neural Inf. Process. Syst.},
number = {NeurIPS},
pages = {6410--6421},
title = {{Graph convolutional policy network for goal-directed molecular graph generation}},
volume = {2018-Decem},
year = {2018}
}
@article{Klicpera2020,
abstract = {Graph neural networks have recently achieved great successes in predicting quantum mechanical properties of molecules. These models represent a molecule as a graph using only the distance between atoms (nodes). They do not, however, consider the spatial direction from one atom to another, despite directional information playing a central role in empirical potentials for molecules, e.g. in angular potentials. To alleviate this limitation we propose directional message passing, in which we embed the messages passed between atoms instead of the atoms themselves. Each message is associated with a direction in coordinate space. These directional message embeddings are rotationally equivariant since the associated directions rotate with the molecule. We propose a message passing scheme analogous to belief propagation, which uses the directional information by transforming messages based on the angle between them. Additionally, we use spherical Bessel functions and spherical harmonics to construct theoretically well-founded, orthogonal representations that achieve better performance than the currently prevalent Gaussian radial basis representations while using fewer than 1/4 of the parameters. We leverage these innovations to construct the directional message passing neural network (DimeNet). DimeNet outperforms previous GNNs on average by 76{\%} on MD17 and by 31{\%} on QM9. Our implementation is available online.},
archivePrefix = {arXiv},
arxivId = {2003.03123},
author = {Klicpera, Johannes and Gro{\ss}, Janek and G{\"{u}}nnemann, Stephan},
eprint = {2003.03123},
file = {::},
pages = {1--13},
title = {{Directional Message Passing for Molecular Graphs}},
url = {http://arxiv.org/abs/2003.03123},
year = {2020}
}
@article{Gomez-Bombarelli2018,
abstract = {We report a method to convert discrete representations of molecules to and from a multidimensional continuous representation. This model allows us to generate new molecules for efficient exploration and optimization through open-ended spaces of chemical compounds. A deep neural network was trained on hundreds of thousands of existing chemical structures to construct three coupled functions: an encoder, a decoder, and a predictor. The encoder converts the discrete representation of a molecule into a real-valued continuous vector, and the decoder converts these continuous vectors back to discrete molecular representations. The predictor estimates chemical properties from the latent continuous vector representation of the molecule. Continuous representations of molecules allow us to automatically generate novel chemical structures by performing simple operations in the latent space, such as decoding random vectors, perturbing known chemical structures, or interpolating between molecules. Continuous representations also allow the use of powerful gradient-based optimization to efficiently guide the search for optimized functional compounds. We demonstrate our method in the domain of drug-like molecules and also in a set of molecules with fewer that nine heavy atoms.},
archivePrefix = {arXiv},
arxivId = {1610.02415},
author = {G{\'{o}}mez-Bombarelli, Rafael and Wei, Jennifer N. and Duvenaud, David and Hern{\'{a}}ndez-Lobato, Jos{\'{e}} Miguel and S{\'{a}}nchez-Lengeling, Benjam{\'{i}}n and Sheberla, Dennis and Aguilera-Iparraguirre, Jorge and Hirzel, Timothy D. and Adams, Ryan P. and Aspuru-Guzik, Al{\'{a}}n and Gómezgómez-Bombarelli, Rafael and Wei, Jennifer N. and Duvenaud, David and {Josémiguel Hernándezhernández-Lobato}, Josémiguel and {Benjam{\'{i}}n Sánchezsánchez-Lengeling}, {\#} and Sheberla, Dennis and Aguilera-Iparraguirre, Jorge and Hirzel, Timothy D. and Adams, Ryan P. and {Alán Aspuru-Guzik}, Alán},
doi = {10.1021/acscentsci.7b00572},
eprint = {1610.02415},
file = {::},
issn = {23747951},
journal = {ACS Cent. Sci.},
number = {2},
pages = {268--276},
publisher = {UTC},
title = {{Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules}},
url = {https://pubs.acs.org/sharingguidelines},
volume = {4},
year = {2018}
}
@article{Xu2018,
abstract = {Recent deep learning approaches for representation learning on graphs follow a neighborhood aggregation procedure. We analyze some important properties of these models, and propose a strategy to overcome those. In particular, the range of "neighboring" nodes that a node's representation draws from strongly depends on the graph structure, analogous to the spread of a random walk. To adapt to local neighborhood properties and tasks, we explore an architecture-jumping knowledge (JK) networks - that flexibly leverages, for each node, different neighborhood ranges to enable better structure-aware representation. In a number of experiments on social, bioinformatics and citation networks, we demonstrate that our model achieves state-of-the-art performance. Furthermore, combining the JK framework with models like Graph Convolutional Networks, GraphSAGE and Graph Attention Networks consistently improves those models' performance.},
archivePrefix = {arXiv},
arxivId = {1806.03536},
author = {Xu, Keyulu and Li, Chengtao and Tian, Yonglong and Sonobe, Tomohiro and Kawarabayashi, Ken Ichi and Jegelka, Stefanie},
eprint = {1806.03536},
file = {::},
isbn = {9781510867963},
journal = {35th Int. Conf. Mach. Learn. ICML 2018},
pages = {8676--8685},
title = {{Representation learning on graphs with jumping knowledge networks}},
volume = {12},
year = {2018}
}
@article{Wang2015,
abstract = {Designing tight-binding ligands is a primary objective of small-molecule drug discovery. Over the past few decades, free-energy calculations have benefited from improved force fields and sampling algorithms, as well as the advent of low-cost parallel computing. However, it has proven to be challenging to reliably achieve the level of accuracy that would be needed to guide lead optimization (5× in binding affinity) for a wide range of ligands and protein targets. Not surprisingly, widespread commercial application of free-energy simulations has been limited due to the lack of large-scale validation coupled with the technical challenges traditionally associated with running these types of calculations. Here, we report an approach that achieves an unprecedented level of accuracy across a broad range of target classes and ligands, with retrospective results encompassing 200 ligands and a wide variety of chemical perturbations, many of which involve significant changes in ligand chemical structures. In addition, we have applied the method in prospective drug discovery projects and found a significant improvement in the quality of the compounds synthesized that have been predicted to be potent. Compounds predicted to be potent by this approach have a substantial reduction in false positives relative to compounds synthesized on the basis of other computational or medicinal chemistry approaches. Furthermore, the results are consistent with those obtained from our retrospective studies, demonstrating the robustness and broad range of applicability of this approach, which can be used to drive decisions in lead optimization.},
author = {Wang, Lingle and Wu, Yujie and Deng, Yuqing and Kim, Byungchan and Pierce, Levi and Krilov, Goran and Lupyan, Dmitry and Robinson, Shaughnessy and Dahlgren, Markus K. and Greenwood, Jeremy and Romero, Donna L. and Masse, Craig and Knight, Jennifer L. and Steinbrecher, Thomas and Beuming, Thijs and Damm, Wolfgang and Harder, Ed and Sherman, Woody and Brewer, Mark and Wester, Ron and Murcko, Mark and Frye, Leah and Farid, Ramy and Lin, Teng and Mobley, David L. and Jorgensen, William L. and Berne, Bruce J. and Friesner, Richard A. and Abel, Robert},
doi = {10.1021/ja512751q},
file = {::},
issn = {15205126},
journal = {J. Am. Chem. Soc.},
number = {7},
pages = {2695--2703},
pmid = {25625324},
title = {{Accurate and reliable prediction of relative ligand binding potency in prospective drug discovery by way of a modern free-energy calculation protocol and force field}},
volume = {137},
year = {2015}
}
@article{Zhou2019,
abstract = {We present a framework, which we call Molecule Deep Q-Networks (MolDQN), for molecule optimization by combining domain knowledge of chemistry and state-of-the-art reinforcement learning techniques (double Q-learning and randomized value functions). We directly define modifications on molecules, thereby ensuring 100{\%} chemical validity. Further, we operate without pre-training on any dataset to avoid possible bias from the choice of that set. MolDQN achieves comparable or better performance against several other recently published algorithms for benchmark molecular optimization tasks. However, we also argue that many of these tasks are not representative of real optimization problems in drug discovery. Inspired by problems faced during medicinal chemistry lead optimization, we extend our model with multi-objective reinforcement learning, which maximizes drug-likeness while maintaining similarity to the original molecule. We further show the path through chemical space to achieve optimization for a molecule to understand how the model works.},
archivePrefix = {arXiv},
arxivId = {1810.08678},
author = {Zhou, Zhenpeng and Kearnes, Steven and Li, Li and Zare, Richard N. and Riley, Patrick},
doi = {10.1038/s41598-019-47148-x},
eprint = {1810.08678},
file = {::;::},
issn = {20452322},
journal = {Sci. Rep.},
keywords = {learning from,molecule optimization,multi-objective optimization,reinforcement learning,scratch},
month = {dec},
number = {1},
publisher = {Nature Publishing Group},
title = {{Optimization of Molecules via Deep Reinforcement Learning}},
volume = {9},
year = {2019}
}
@article{Meng2017,
abstract = {Rechargeable batteries undoubtedly represent one of the best candidates for chemical energy storage, where the intrinsic structures of electrode materials play a crucial role in understanding battery chemistry and improving battery performance. This review emphasizes the advances in structure and property optimizations of battery electrode materials for high-efficiency energy storage. The underlying battery reaction mechanisms of insertion-, conversion-, and alloying-type materials are first discussed toward rational battery designs. We then give a summary of the advanced optimization strategies and provide in-depth analyses of structure-property relationships for some significant research breakthroughs in batteries. Finally, we provide a brief overview of the recent state-of-the-art techniques as powerful tools to explore and predict the strategies of structure optimization. This review on the deep understanding of structure-property correlations will bring new insight into the design of ideal battery materials and open up new opportunities for battery chemistry. Increasing energy demands for potential portable electronics, electric vehicles, and smart power grids have stimulated intensive efforts to develop highly efficient rechargeable batteries for chemical energy storage. The intrinsic structures of electrode materials are crucial in understanding battery chemistry and improving battery performance for large-scale applications. This review presents a new insight by summarizing the advances in structure and property optimizations of battery electrode materials for high-efficiency energy storage. In-depth understanding, efficient optimization strategies, and advanced techniques on electrode materials are also highlighted. By now, many challenges still exist for achieving great breakthroughs in high-performance scalable batteries. To address the challenges, some important aspects are proposed on the development of battery research in the future. First, it is highly desired to develop new electrode materials and new storage devices to meet the urgent energy demands for large-scale applications. Second, the in-depth understanding of the fundamental reaction mechanisms and the structure-property correlations is essential. Third, it is critical to explore low-cost electrode materials and fabrication processes for large-scale applications in industry. Moreover, bridging the fundamental research in laboratory and the scalable development in industry is extremely important in directing lab-based research toward practical applications. Based on the in-depth understanding of battery chemistry in electrode materials, some important reaction mechanisms and design principles are clearly revealed, and the strategies for structure optimizations toward high-performance batteries are summarized. This review will provide a suitable pathway toward the rational design of ideal battery materials for large-scale applications in industry and open up new opportunities for battery chemistry. The increase of energy demands for potential portable electronics, electric vehicles, and smart power grids requires the batteries to have improved safety, higher energy/power density, longer cycle life, and lower cost. This review covers in-depth discussions of the battery reaction mechanisms and advanced techniques and highlights the structure and property optimizations of battery materials for high-efficiency energy storage.},
author = {Meng, Jiashen and Guo, Haichang and Niu, Chaojiang and Zhao, Yunlong and Xu, Lin and Li, Qi and Mai, Liqiang},
doi = {10.1016/j.joule.2017.08.001},
file = {::},
issn = {25424351},
journal = {Joule},
keywords = {advanced techniques,battery electrode materials,high-efficiency energy storage,structure and property optimizations},
number = {3},
pages = {522--547},
title = {{Advances in Structure and Property Optimizations of Battery Electrode Materials}},
volume = {1},
year = {2017}
}
@article{Yang2019,
abstract = {Advancements in neural machinery have led to a wide range of algorithmic solutions for molecular property prediction. Two classes of models in particular have yielded promising results: neural networks applied to computed molecular fingerprints or expert-crafted descriptors and graph convolutional neural networks that construct a learned molecular representation by operating on the graph structure of the molecule. However, recent literature has yet to clearly determine which of these two methods is superior when generalizing to new chemical space. Furthermore, prior research has rarely examined these new models in industry research settings in comparison to existing employed models. In this paper, we benchmark models extensively on 19 public and 16 proprietary industrial data sets spanning a wide variety of chemical end points. In addition, we introduce a graph convolutional model that consistently matches or outperforms models using fixed molecular descriptors as well as previous graph neural architectures on both public and proprietary data sets. Our empirical findings indicate that while approaches based on these representations have yet to reach the level of experimental reproducibility, our proposed model nevertheless offers significant improvements over models currently used in industrial workflows.},
archivePrefix = {arXiv},
arxivId = {1904.01561},
author = {Yang, Kevin and Swanson, Kyle and Jin, Wengong and Coley, Connor and Eiden, Philipp and Gao, Hua and Guzman-Perez, Angel and Hopper, Timothy and Kelley, Brian and Mathea, Miriam and Palmer, Andrew and Settels, Volker and Jaakkola, Tommi and Jensen, Klavs and Barzilay, Regina},
doi = {10.1021/acs.jcim.9b00237},
eprint = {1904.01561},
file = {::},
issn = {15205142},
journal = {J. Chem. Inf. Model.},
number = {8},
pages = {3370--3388},
publisher = {American Chemical Society},
title = {{Analyzing Learned Molecular Representations for Property Prediction}},
volume = {59},
year = {2019}
}
@article{Fratev,
abstract = {Recent improvements to the free energy perturbation (FEP) calculations, especially FEP+ , established their utility for pharmaceutical lead optimization. Herein, we propose a modified version of the FEP/ REST (i.e., replica exchange with solute tempering) sampling protocol, based on detail studies on several targets by probing a large number of perturbations with different sampling schemes. Improved fep+ binding affinity predictions for regular flexible-loop motions and considerable structural changes can be obtained by extending the prior to REST (pre-REST) sampling time from 0.24 ns/$\lambda$ to 5 ns/$\lambda$ and 2 × 10 ns/$\lambda$, respectively. With this new protocol, much more precise ∆∆G values of the individual perturbations, including the sign of the transformations and decreased error were obtained. We extended the REST simulations from 5 ns to 8 ns to achieve reasonable free energy convergence. Implementing REST to the entire ligand as opposed to solely the perturbed region, and also some important flexible protein residues (pREST region) in the ligand binding domain (LBD) has considerably improved the fep+ results in most of the studied cases. preliminary molecular dynamics (MD) runs were useful for establishing the correct binding mode of the compounds and thus precise alignment for fep+. our improved protocol may further increase the fep+ accuracy. Accurate in silico predictions of ligand-protein binding affinities continues to be a primary objective of structure-based pharmaceutical design because of its putative value for drug discovery. Improvements to binding affinity and selectivity are critical to hit-to-lead optimization efforts. Free energy perturbation (FEP) calculations are attractive for predicting ligand-protein binding affinities via molecular simulations as well as for reducing the duration of the lead optimization phase of pharmaceutical development, which is as an individual stage the most expensive part of drug discovery 1,2. Due to increased graphics processing unit (GPU) computational power the applications of FEP, especially FEP+ , has recently become very popular in both conventional lead and fragment optimization 3-7. In general, FEP+ shows considerable correlations between calculated and experimental binding free energies, and average errors in the range of only 1 kcal/mol 3. FEP calculations are based on molecular dynamics (MD) simulations and therefore explicitly consider both enthalpy and entropy effects of the conformational flexibility of the ligand, as well as desolvation effects within the ligand binding domain (LBD) of certain receptors. However, to date FEP has typically been helpful mainly when (1) high-quality X-ray data is available and (2) the target protein does not undergo significant conformational changes. Also, a lack of detail studies on determining an adequate sampling time is often one of the primary limitations of FEP calculations. Further, FEP remains insufficient in several aspects and requires improvement. Detection of the most likely ligand binding mode, the presence of multiple stable binding conformations, insufficient equilibration, and determining an adequate sampling time (especially when significant protein side chain and backbone residue flexibility is possible) are the most critical aspects of FEP, and have been recently reviewed in detail 8. Although these problems can be at least partially resolved by execution of reasonably long (≈100-300 ns) preliminary MD simulations, such an approach is often underutilized in applied simulations 8-10. Many failures of FEP calculations are attributable to underestimating the importance of the preliminary MD studies 8. Recently, we proposed a common computational workflow which can retrieve accurate information about the ligand binding modes and the starting pose for FEP+ simulations 11. We recommend using this workflow before setting up and executing},
author = {Fratev, Filip and Sirimulla, Suman},
doi = {10.1038/s41598-019-53133-1},
file = {::},
title = {{An improved free energy perturbation fep+ Sampling protocol for flexible Ligand-Binding Domains}}
}
@article{Stokes2020,
abstract = {Due to the rapid emergence of antibiotic-resistant bacteria, there is a growing need to discover new antibiotics. To address this challenge, we trained a deep neural network capable of predicting molecules with antibacterial activity. We performed predictions on multiple chemical libraries and discovered a molecule from the Drug Repurposing Hub—halicin—that is structurally divergent from conventional antibiotics and displays bactericidal activity against a wide phylogenetic spectrum of pathogens including Mycobacterium tuberculosis and carbapenem-resistant Enterobacteriaceae. Halicin also effectively treated Clostridioides difficile and pan-resistant Acinetobacter baumannii infections in murine models. Additionally, from a discrete set of 23 empirically tested predictions from {\textgreater}107 million molecules curated from the ZINC15 database, our model identified eight antibacterial compounds that are structurally distant from known antibiotics. This work highlights the utility of deep learning approaches to expand our antibiotic arsenal through the discovery of structurally distinct antibacterial molecules. A trained deep neural network predicts antibiotic activity in molecules that are structurally different from known antibiotics, among which Halicin exhibits efficacy against broad-spectrum bacterial infections in mice.},
author = {Stokes, Jonathan M. and Yang, Kevin and Swanson, Kyle and Jin, Wengong and Cubillos-Ruiz, Andres and Donghia, Nina M. and MacNair, Craig R. and French, Shawn and Carfrae, Lindsey A. and Bloom-Ackerman, Zohar and Tran, Victoria M. and Chiappino-Pepe, Anush and Badran, Ahmed H. and Andrews, Ian W. and Chory, Emma J. and Church, George M. and Brown, Eric D. and Jaakkola, Tommi S. and Barzilay, Regina and Collins, James J.},
doi = {10.1016/j.cell.2020.01.021},
file = {::},
issn = {10974172},
journal = {Cell},
keywords = {antibiotic resistance,antibiotic tolerance,antibiotics,drug discovery,machine learning},
number = {4},
pages = {688--702.e13},
publisher = {Elsevier Inc.},
title = {{A Deep Learning Approach to Antibiotic Discovery}},
url = {https://doi.org/10.1016/j.cell.2020.01.021},
volume = {180},
year = {2020}
}
@article{Jin2020,
abstract = {Drug discovery aims to find novel compounds with specified chemical property profiles. In terms of generative modeling, the goal is to learn to sample molecules in the intersection of multiple property constraints. This task becomes increasingly challenging when there are many property constraints. We propose to offset this complexity by composing molecules from a vocabulary of substructures that we call molecular rationales. These rationales are identified from molecules as substructures that are likely responsible for each property of interest. We then learn to expand rationales into a full molecule using graph generative models. Our final generative model composes molecules as mixtures of multiple rationale completions, and this mixture is fine-tuned to preserve the properties of interest. We evaluate our model on various drug design tasks and demonstrate significant improvements over state-of-the-art baselines in terms of accuracy, diversity, and novelty of generated compounds.},
archivePrefix = {arXiv},
arxivId = {2002.03244},
author = {Jin, Wengong and Barzilay, Regina and Jaakkola, Tommi},
eprint = {2002.03244},
file = {::},
month = {feb},
title = {{Multi-Objective Molecule Generation using Interpretable Substructures}},
url = {http://arxiv.org/abs/2002.03244},
year = {2020}
}
@article{Wang2019a,
abstract = {Point cloud registration is a key problem for computer vision applied to robotics, medical imaging, and other applications. This problem involves finding a rigid transformation from one point cloud into another so that they align. Iterative Closest Point (ICP) and its variants provide simple and easily-implemented iterative methods for this task, but these algorithms can converge to spurious local optima. To address local optima and other difficulties in the ICP pipeline, we propose a learning-based method, titled Deep Closest Point (DCP), inspired by recent techniques in computer vision and natural language processing. Our model consists of three parts: A point cloud embedding network, an attention-based module combined with a pointer generation layer to approximate combinatorial matching, and a differentiable singular value decomposition (SVD) layer to extract the final rigid transformation. We train our model end-to-end on the ModelNet40 dataset and show in several settings that it performs better than ICP, its variants (e.g., Go-ICP, FGR), and the recently-proposed learning-based method PointNetLK. Beyond providing a state-of-the-art registration technique, we evaluate the suitability of our learned features transferred to unseen objects. We also provide preliminary analysis of our learned model to help understand whether domain-specific and/or global features facilitate rigid registration.},
archivePrefix = {arXiv},
arxivId = {1905.03304},
author = {Wang, Yue and Solomon, Justin},
doi = {10.1109/ICCV.2019.00362},
eprint = {1905.03304},
file = {::},
isbn = {9781728148038},
issn = {15505499},
journal = {Proc. IEEE Int. Conf. Comput. Vis.},
pages = {3522--3531},
title = {{Deep closest point: Learning representations for point cloud registration}},
volume = {2019-Octob},
year = {2019}
}
@article{Krueger2016,
abstract = {Active reinforcement learning (ARL) is a variant on reinforcement learning where the agent does not observe the reward unless it chooses to pay a query cost c {\textgreater} 0. The central question of ARL is how to quantify the long-term value of reward information. Even in multi-armed bandits, computing the value of this information is intractable and we have to rely on heuristics. We propose and evaluate several heuristic approaches for ARL in multi-armed bandits and (tabular) Markov decision processes, and discuss and illustrate some challenging aspects of the ARL problem.},
author = {Krueger, David and Leike, Jan and Evans, Owain and Salvatier, John},
file = {::},
journal = {Nips},
number = {Nips},
title = {{Active Reinforcement Learning: Observing Rewards at a Cost}},
year = {2016}
}
@article{Simm2020,
abstract = {Automating molecular design using deep reinforcement learning (RL) holds the promise of accelerating the discovery of new chemical compounds. A limitation of existing approaches is that they work with molecular graphs and thus ignore the location of atoms in space, which restricts them to 1) generating single organic molecules and 2) heuristic reward functions. To address this, we present a novel RL formulation for molecular design in Cartesian coordinates, thereby extending the class of molecules that can be built. Our reward function is directly based on fundamental physical properties such as the energy, which we approximate via fast quantum-chemical methods. To enable progress towards de-novo molecular design, we introduce MolGym, an RL environment comprising several challenging molecular design tasks along with baselines. In our experiments, we show that our agent can efficiently learn to solve these tasks from scratch by working in a translation and rotation invariant state-action space.},
archivePrefix = {arXiv},
arxivId = {2002.07717},
author = {Simm, Gregor N. C. and Pinsler, Robert and Hern{\'{a}}ndez-Lobato, Jos{\'{e}} Miguel},
eprint = {2002.07717},
file = {::},
title = {{Reinforcement Learning for Molecular Design Guided by Quantum Mechanics}},
url = {http://arxiv.org/abs/2002.07717},
year = {2020}
}
@article{Gilmer2017,
abstract = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.},
archivePrefix = {arXiv},
arxivId = {arXiv:1704.01212v2},
author = {Gilmer, Justin and Schoenholz, Samuel S. and Riley, Patrick F. and Vinyals, Oriol and Dahl, George E.},
eprint = {arXiv:1704.01212v2},
file = {::},
isbn = {9781510855144},
journal = {34th Int. Conf. Mach. Learn. ICML 2017},
pages = {2053--2070},
title = {{Neural message passing for quantum chemistry}},
volume = {3},
year = {2017}
}
@article{Shi2020,
abstract = {A fundamental problem in computational chemistry is to find a set of reactants to synthesize a target molecule, a.k.a. retrosynthesis prediction. Existing state-of-the-art methods rely on matching the target molecule with a large set of reaction templates, which are very computationally expensive and also suffer from the problem of coverage. In this paper, we propose a novel template-free approach called G2Gs by transforming a target molecular graph into a set of reactant molecular graphs. G2Gs first splits the target molecular graph into a set of synthons by identifying the reaction centers, and then translates the synthons to the final reactant graphs via a variational graph translation framework. Experimental results show that G2Gs significantly outperforms existing template-free approaches by up to 63{\%} in terms of the top-1 accuracy and achieves a performance close to that of state-of-the-art template based approaches, but does not require domain knowledge and is much more scalable.},
archivePrefix = {arXiv},
arxivId = {2003.12725},
author = {Shi, Chence and Xu, Minkai and Guo, Hongyu and Zhang, Ming and Tang, Jian},
eprint = {2003.12725},
file = {::},
number = {2},
title = {{A Graph to Graphs Framework for Retrosynthesis Prediction}},
url = {http://arxiv.org/abs/2003.12725},
year = {2020}
}
@article{Morris2019,
abstract = {In recent years, graph neural networks (GNNs) have emerged as a powerful neural architecture to learn vector representations of nodes and graphs in a supervised, end-to-end fashion. Up to now, GNNs have only been evaluated empirically—showing promising results. The following work investigates GNNs from a theoretical point of view and relates them to the 1-dimensional Weisfeiler-Leman graph isomorphism heuristic (1-WL). We show that GNNs have the same expressiveness as the 1-WL in terms of distinguishing non-isomorphic (sub-)graphs. Hence, both algorithms also have the same shortcomings. Based on this, we propose a generalization of GNNs, so-called k-dimensional GNNs (k-GNNs), which can take higher-order graph structures at multiple scales into account. These higher-order structures play an essential role in the characterization of social networks and molecule graphs. Our experimental evaluation confirms our theoretical findings as well as confirms that higher-order information is useful in the task of graph classification and regression.},
archivePrefix = {arXiv},
arxivId = {1810.02244},
author = {Morris, Christopher and Ritzert, Martin and Fey, Matthias and Hamilton, William L. and Lenssen, Jan Eric and Rattan, Gaurav and Grohe, Martin},
doi = {10.1609/aaai.v33i01.33014602},
eprint = {1810.02244},
file = {::},
issn = {2159-5399},
journal = {Proc. AAAI Conf. Artif. Intell.},
pages = {4602--4609},
title = {{Weisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks}},
volume = {33},
year = {2019}
}
@article{Sun2019,
abstract = {This paper studies learning the representations of whole graphs in both unsupervised and semi-supervised scenarios. Graph-level representations are critical in a variety of real-world applications such as predicting the properties of molecules and community analysis in social networks. Traditional graph kernel based methods are simple, yet effective for obtaining fixed-length representations for graphs but they suffer from poor generalization due to hand-crafted designs. There are also some recent methods based on language models (e.g. graph2vec) but they tend to only consider certain substructures (e.g. subtrees) as graph representatives. Inspired by recent progress of unsupervised representation learning, in this paper we proposed a novel method called InfoGraph for learning graph-level representations. We maximize the mutual information between the graph-level representation and the representations of substructures of different scales (e.g., nodes, edges, triangles). By doing so, the graph-level representations encode aspects of the data that are shared across different scales of substructures. Furthermore, we further propose InfoGraph*, an extension of InfoGraph for semi-supervised scenarios. InfoGraph* maximizes the mutual information between unsupervised graph representations learned by InfoGraph and the representations learned by existing supervised methods. As a result, the supervised encoder learns from unlabeled data while preserving the latent semantic space favored by the current supervised task. Experimental results on the tasks of graph classification and molecular property prediction show that InfoGraph is superior to state-of-the-art baselines and InfoGraph* can achieve performance competitive with state-of-the-art semi-supervised models.},
archivePrefix = {arXiv},
arxivId = {1908.01000},
author = {Sun, Fan-Yun and Hoffmann, Jordan and Verma, Vikas and Tang, Jian},
eprint = {1908.01000},
file = {:Users/Simon/Documents/papiers{\_}mendeley/2019 - Sun et al. - InfoGraph Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization.pdf:pdf},
number = {2018},
pages = {1--16},
title = {{InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization}},
url = {http://arxiv.org/abs/1908.01000},
year = {2019}
}
@article{Mansimov2019,
abstract = {A molecule's geometry, also known as conformation, is one of a molecule's most important properties, determining the reactions it participates in, the bonds it forms, and the interactions it has with other molecules. Conventional conformation generation methods minimize hand-designed molecular force field energy functions that are often not well correlated with the true energy function of a molecule observed in nature. They generate geometrically diverse sets of conformations, some of which are very similar to the lowest-energy conformations and others of which are very different. In this paper, we propose a conditional deep generative graph neural network that learns an energy function by directly learning to generate molecular conformations that are energetically favorable and more likely to be observed experimentally in data-driven manner. On three large-scale datasets containing small molecules, we show that our method generates a set of conformations that on average is far more likely to be close to the corresponding reference conformations than are those obtained from conventional force field methods. Our method maintains geometrical diversity by generating conformations that are not too similar to each other, and is also computationally faster. We also show that our method can be used to provide initial coordinates for conventional force field methods. On one of the evaluated datasets we show that this combination allows us to combine the best of both methods, yielding generated conformations that are on average close to reference conformations with some very similar to reference conformations.},
archivePrefix = {arXiv},
arxivId = {1904.00314},
author = {Mansimov, Elman and Mahmood, Omar and Kang, Seokho and Cho, Kyunghyun},
doi = {10.1038/s41598-019-56773-5},
eprint = {1904.00314},
file = {::},
issn = {20452322},
journal = {Sci. Rep.},
number = {1},
pages = {1--13},
pmid = {31892716},
publisher = {Springer US},
title = {{Molecular Geometry Prediction using a Deep Generative Graph Neural Network}},
url = {http://dx.doi.org/10.1038/s41598-019-56773-5},
volume = {9},
year = {2019}
}
